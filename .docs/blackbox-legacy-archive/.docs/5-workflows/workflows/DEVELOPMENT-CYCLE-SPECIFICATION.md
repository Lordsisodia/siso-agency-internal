# Black Box Automated UI Development Cycle
## Agent-Executable Specification

---

## üéØ OVERVIEW

This document outlines a fully automated UI iteration cycle for Black Box development, designed to be executed by AI agents with minimal human intervention.

---

## üìã PHASE 1: TASK IDENTIFICATION & PLANNING

### 1.1 Research Codebase
**Goal:** Find all related code and understand current implementation

**Actions:**
- Use semantic search (grep/find) to locate relevant files
- Identify component hierarchy and dependencies
- Map data flow and state management
- Document existing patterns and conventions
- Check for similar implementations that can be referenced

**Outputs:**
- File inventory (list of relevant files)
- Dependency graph
- Pattern documentation
- Potential conflict points

### 1.2 Evaluate Current State
**Goal:** Understand what exists and what needs to change

**Actions:**
- Run existing tests to establish baseline
- Check current UI state (screenshots, accessibility tree)
- Review code quality metrics
- Identify technical debt or refactoring opportunities
- Assess performance implications

**Outputs:**
- Baseline test results
- Current state screenshots
- Technical assessment
- Risk evaluation

### 1.3 Determine Implementation Approaches
**Goal:** Explore multiple viable solutions

**Actions:**
- Brainstorm 2-3 different implementation strategies
- Evaluate trade-offs for each approach:
  - Development time
  - Code complexity
  - Performance impact
  - Maintainability
  - Backward compatibility
- Select optimal approach with rationale

**Outputs:**
- Implementation options (2-3 approaches)
- Trade-off analysis matrix
- Recommended approach with justification

---

## üõ†Ô∏è PHASE 2: EXECUTION & TESTING

### 2.1 Write Code
**Goal:** Implement the chosen solution

**Actions:**
- Create new components or modify existing ones
- Follow established code patterns and conventions
- Write self-documenting code with clear naming
- Add inline comments for complex logic
- Ensure TypeScript type safety
- Maintain consistent code style

**Quality Checks:**
- ‚úÖ No linting errors
- ‚úÖ No TypeScript errors
- ‚úÖ Follows project conventions
- ‚úÖ Proper error handling
- ‚úÖ Accessible markup (ARIA labels, semantic HTML)

**Outputs:**
- New/modified code files
- Updated tests if needed
- Code diff summary

### 2.2 Automated UI Testing
**Goal:** Verify functionality and visual correctness across devices

**Testing Tools:**
- Chrome DevTools MCP (for visual inspection)
- Playwright (for automated browser testing)
- Accessibility tree analysis

**Test Coverage:**

#### Desktop Testing (1920x1080, 1366x768):
- ‚úÖ Visual regression (screenshots)
- ‚úÖ Interactive elements work (clicks, inputs)
- ‚úÖ Responsive layout behaves correctly
- ‚úÖ No console errors or warnings
- ‚úÖ Accessibility tree is complete
- ‚úÖ Keyboard navigation works
- ‚úÖ Text contrast ratios meet WCAG AA

#### Mobile Testing (375x667, 414x896):
- ‚úÖ Touch targets are minimum 44x44px
- ‚úÖ Layout adapts to small screens
- ‚úÖ No horizontal scrolling
- ‚úÖ Readable text sizes (min 16px)
- ‚úÖ Proper viewport meta tag
- ‚úÖ No hover-only interactions

#### Cross-Browser Testing:
- ‚úÖ Chrome/Edge (Chromium)
- ‚úÖ Firefox
- ‚úÖ Safari (if applicable)

**Test Results:**
- Screenshot comparison (before/after)
- Console error log
- Accessibility audit report
- Performance metrics (FCP, LCP, TTI)
- Touch/device capability report

### 2.3 Quality Gates
**Goal:** Ensure production readiness

**Criteria:**
- ‚úÖ All tests passing
- ‚úÖ Zero console errors
- ‚úÖ No visual regressions
- ‚úÖ Accessibility score > 90
- ‚úÖ Performance within acceptable bounds
- ‚úÖ Code review checklist passed

**If ANY gate fails ‚Üí Loop back to Phase 2.1**

**Outputs:**
- Test results summary
- Quality gate status (PASS/FAIL)
- Failure details (if any)

---

## üöÄ PHASE 3: DEPLOYMENT LOOP

### 3.1 Development Deployment
**Goal:** Deploy to development environment for staging

**Actions:**
- Commit changes with descriptive message
- Push to `dev` branch on GitHub
- Trigger automated CI/CD pipeline
- Verify deployment success
- Update deployment tracking

**Outputs:**
- Git commit SHA
- Deployment URL (dev environment)
- CI/CD pipeline status
- Deployment log

### 3.2 Staging Verification
**Goal:** Verify in development environment

**Actions:**
- Navigate to dev URL
- Run smoke tests (critical user flows)
- Check browser console for errors
- Verify API integrations work
- Test authentication if applicable
- Record any environment-specific issues

**If staging fails ‚Üí Loop back to Phase 2.1**

**Outputs:**
- Smoke test results
- Console error report
- Environment issue log
- Staging gate status (PASS/FAIL)

### 3.3 Production Deployment
**Goal:** Deploy to production for real-world validation

**Actions:**
- Merge dev branch to `main`
- Trigger Cloudflare deployment
- Monitor deployment logs
- Verify DNS propagation
- Check SSL certificates
- Confirm production URL is accessible

**Outputs:**
- Production URL
- Cloudflare deployment status
- DNS verification results
- Deployment timestamp

### 3.4 Production Verification
**Goal:** Ensure production is fully operational

**Critical Checks:**
- ‚úÖ Production URL loads correctly
- ‚úÖ All critical user flows work
- ‚úÖ Browser console is clean (no errors)
- ‚úÖ Third-party integrations functional
- ‚úÖ Analytics firing correctly
- ‚úÖ Performance metrics acceptable
- ‚úÖ Error tracking (Sentry/etc.) shows no new issues

**Actions:**
- Load production URL in multiple browsers
- Test critical user journeys
- Monitor browser console
- Check API endpoints
- Verify analytics/events
- Review error tracking dashboard
- Test on real devices (if possible)

**If production fails ‚Üí Rollback + Loop back to Phase 2.1**

**Outputs:**
- Production verification report
- Browser console log
- Error tracking summary
- Rollback status (if applicable)
- Production gate status (PASS/FAIL)

---

## üîÑ LOOP BEHAVIOR

### Success Path
```
Phase 1 ‚Üí Phase 2 ‚Üí Phase 3 ‚Üí ‚úÖ COMPLETE
```

### Failure Loops

#### Development Loop
```
Phase 2.1 (Code) ‚Üí Phase 2.2 (Test) ‚Üí ‚ùå FAIL ‚Üí Phase 2.1
```

#### Staging Loop
```
Phase 3.1 (Dev Deploy) ‚Üí Phase 3.2 (Staging Verify) ‚Üí ‚ùå FAIL ‚Üí Phase 2.1
```

#### Production Loop
```
Phase 3.3 (Prod Deploy) ‚Üí Phase 3.4 (Prod Verify) ‚Üí ‚ùå FAIL ‚Üí ROLLBACK ‚Üí Phase 2.1
```

### Loop Limits
- **Max development loops:** 3 (then escalate to human)
- **Max staging loops:** 2 (then escalate to human)
- **Max production loops:** 1 (then escalate to human + rollback)

---

## ‚ö†Ô∏è CRITICAL MISSING CRITERIA

### 1. **Data Migration & State Management**
- ‚ùå What happens to existing user data when schema changes?
- ‚ùå Database migration planning
- ‚ùå State compatibility checks
- ‚ùå Rollback data strategies

### 2. **Error Handling & Edge Cases**
- ‚ùå Network failure handling
- ‚ùå API timeout strategies
- ‚ùå Offline mode considerations
- ‚ùå Boundary condition testing

### 3. **Security Considerations**
- ‚ùå Authentication/authorization testing
- ‚ùå XSS/injection checks
- ‚ùå CSP policy validation
- ‚ùå Sensitive data handling
- ‚ùå API key security

### 4. **Performance Metrics**
- ‚ùå Specific performance budgets (e.g., FCP < 1.8s)
- ‚ùå Bundle size impact analysis
- ‚ùå Memory leak detection
- ‚ùå Lazy loading verification

### 5. **SEO & Analytics**
- ‚ùå Meta tag updates
- ‚ùå Open Graph tags
- ‚ùå Structured data validation
- ‚ùå Analytics events firing
- ‚ùå Tracking implementation verification

### 6. **Backward Compatibility**
- ‚ùå API versioning
- ‚ùå Feature flags for gradual rollout
- ‚ùå Deprecation warnings
- ‚ùå Migration path for existing users

### 7. **Documentation**
- ‚ùå Update README/API docs
- ‚ùå Document breaking changes
- ‚ùå Update changelog
- ‚ùå Component documentation (Storybook?)

### 8. **Monitoring & Alerting**
- ‚ùå Set up uptime monitoring
- ‚ùå Configure error alerts
- ‚ùå Performance monitoring dashboards
- ‚ùå Custom event tracking

### 9. **Regression Testing**
- ‚ùå Run full test suite, not just new tests
- ‚ùå Check for unintended side effects
- ‚ùå Verify existing features still work
- ‚ùå Cross-feature interaction testing

### 10. **Human Escalation Triggers**
- ‚ùå When to get human review (ambiguous requirements)
- ‚ùå Security vulnerability discovered
- ‚ùå Performance degradation beyond threshold
- ‚ùå Cannot resolve after max loops
- ‚ùå Dependencies have critical vulnerabilities

### 11. **Environment Configuration**
- ‚ùå Environment variable management
- ‚ùå Config file differences (dev/staging/prod)
- ‚ùå Secret management verification
- ‚ùå API endpoint configuration

### 12. **Accessibility Compliance**
- ‚ùå Screen reader testing (NVDA/JAWS)
- ‚ùå Keyboard-only navigation
- ‚ùå Focus management
- ‚ùå ARIA live regions
- ‚ùå Color contrast verification

### 13. **Internationalization (if applicable)**
- ‚ùå Text extraction for translation
- ‚ùå RTL language support
- ‚ùå Date/time formatting
- ‚ùå Currency formatting
- ‚ùå Character encoding

---

## üìä SUCCESS CRITERIA SUMMARY

### Phase Gates
Each phase must pass explicit gates before proceeding:

**Phase 1 Gate:**
- ‚úÖ Research complete
- ‚úÖ Current state documented
- ‚úÖ Implementation approach selected

**Phase 2 Gate:**
- ‚úÖ Code written (no lint/type errors)
- ‚úÖ All tests passing
- ‚úÖ QA checks passed

**Phase 3 Gate:**
- ‚úÖ Staging verified
- ‚úÖ Production deployed
- ‚úÖ Production verified

### Final Success Indicators
- ‚úÖ Zero console errors in production
- ‚úÖ All critical user flows working
- ‚úÖ Performance metrics within budget
- ‚úÖ Accessibility score > 90
- ‚úÖ No new errors in tracking
- ‚úÖ Monitoring shows normal operation

---

## ü§ñ AGENT EXECUTION REQUIREMENTS

### Prerequisites
- Access to codebase (read/write)
- Git credentials
- Browser automation tools (Chrome MCP/Playwright)
- CI/CD pipeline access
- Monitoring tools access

### Required Capabilities
- Semantic code search
- File modification
- Test execution
- Screenshot capture
- Console log analysis
- Git operations
- Deployment verification
- Error tracking access

### Communication Protocol
- Report phase completion
- Flag blockers immediately
- Summarize test results
- Request human escalation when needed
- Provide rollback rationale if applicable

---

## üìù NEXT STEPS

1. **Review this specification** - Are all criteria accounted for?
2. **Define project-specific thresholds** - Performance budgets, acceptance criteria
3. **Set up monitoring infrastructure** - Before running first cycle
4. **Create escalation protocols** - When and how to get human help
5. **Test the cycle** - Run a pilot with a simple task
6. **Iterate on the process** - Refine based on learnings

---

## üéØ QUESTIONS FOR CLARIFICATION

1. What is the rollback strategy if production deployment fails?
2. Should there be a canary deployment phase?
3. How do we handle database schema changes?
4. What are the specific performance budgets?
5. Should there be A/B testing capabilities?
6. How do we handle feature flags?
7. What's the process for hotfixes (critical bugs)?
8. Should there be a code review step before deployment?
9. How do we handle dependencies that need updating?
10. What's the strategy for mobile app vs web app?

---

*This specification is a living document. Update as the cycle is tested and refined.*
