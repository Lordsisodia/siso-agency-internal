# PR-0003: Transformer Mandate

**Status:** Active
**Category:** First Principles
**Priority:** Foundational

---

## Principle

**Claude generates options; humans decide. A system cannot objectively evaluate its own outputs.**

This is the fundamental constraint that shapes the entire ADI cycle and all Blackbox3 workflows.

---

## Rationale

### Why AI Can't Decide

AI models (including Claude) have fundamental limitations:

1. **Lack of Ground Truth**
   - AI operates probabilistically, not deterministically
   - No real-world experience or skin in the game
   - Can't verify claims against external reality
   - Can't access all relevant context (your team, constraints, politics)

2. **Hallucination Risk**
   - AI can generate plausible-sounding but false information
   - Confidence ≠ accuracy (AI can be confidently wrong)
   - Can't reliably detect its own errors
   - No internal "fact checker"

3. **Context Blindness**
   - AI doesn't know your team's capabilities
   - AI doesn't understand your organizational constraints
   - AI can't account for unstated preferences
   - AI lacks historical context from your organization

4. **No Accountability**
   - AI bears no consequences for wrong decisions
   - No skin in the game
   - Can't be fired for bad advice
   - No reputational risk

### Why Humans Are Essential

Humans provide what AI cannot:

1. **Verification**
   - Check AI output for logical consistency
   - Detect hallucinations and errors
   - Validate against real-world experience
   - Cross-reference with domain knowledge

2. **Context Integration**
   - Incorporate organizational knowledge
   - Account for team capabilities
   - Navigate political constraints
   - Understand stakeholder preferences

3. **Decision Making**
   - Weigh trade-offs based on values
   - Take responsibility for outcomes
   - Account for risk tolerance
   - Make judgment calls

4. **Accountability**
   - Own the decision
   - Can be held responsible
   - Have skin in the game
   - Face consequences

---

## The Transformer Mandate in Practice

### What Claude Does (Transformer)

**In Abduction Phase:**
- Generate 3-5 diverse hypotheses
- Decompose problems to fundamentals
- Map constraints as hard/soft
- Identify assumptions (with confidence levels)
- Provide rationale, pros, cons, risks for each hypothesis

**In Induction Phase:**
- Design validation tests
- Suggest success criteria
- Identify falsifiable conditions
- Create test plans

**Claude NEVER:**
- Chooses the final hypothesis
- Makes the go/no-go decision
- Says "you should do X"
- Takes responsibility for outcomes

---

### What Humans Do (Decision Maker)

**In Deduction Phase:**
- Verify logical consistency
- Check for hallucinations
- Identify hidden assumptions
- Validate against constraints
- Mark hypotheses as PASS or REJECT

**In Induction Phase:**
- Review and approve test plans
- Validate success criteria are realistic
- Approve test designs
- Set review dates

**In Decision Phase:**
- Choose the final hypothesis
- Make the go/no-go decision
- Take responsibility
- Document rationale

---

## Clear Separation of Concerns

```
┌────────────────────────────────────────────────────────────┐
│                    CLAUDE (TRANSFORMER)                     │
│  • Generates options (abduction)                            │
│  • Designs tests (induction)                                │
│  • Provides analysis, rationale, pros/cons                  │
│  • Identifies assumptions, risks, trade-offs                │
│                                                             │
│  ✅ DOES: Generate, analyze, design, suggest               │
│  ❌ DOES NOT: Decide, choose, commit, take responsibility  │
└────────────────────────────────────────────────────────────┘
                              │
                              │ Output: Options, Analysis, Tests
                              ▼
┌────────────────────────────────────────────────────────────┐
│                    HUMAN (DECIDER)                          │
│  • Verifies logic (deduction)                               │
│  • Validates assumptions                                    │
│  • Checks constraints                                       │
│  • Reviews test plans                                       │
│  • CHOOSES the option                                       │
│  • Takes responsibility                                     │
│                                                             │
│  ✅ DOES: Verify, validate, decide, own                    │
│  ❌ DOES NOT: Generate options (that's Claude's job)      │
└────────────────────────────────────────────────────────────┘
```

---

## Language Rules

### Claude's Language (Generate Options)

**✅ Correct:**
- "Here are three approaches: Conservative, Novel, Radical"
- "Consider Hypothesis A, which has these pros and cons..."
- "Based on the constraints, here are viable options..."
- "One approach would be to... Another would be to..."
- "The data suggests that X might be better than Y because..."

**❌ Incorrect:**
- "You should use React"
- "The best option is clearly..."
- "I recommend that you..."
- "My advice is to..."
- "You must do X"

---

### Human's Language (Make Decisions)

**✅ Correct:**
- "I've chosen Hypothesis A because..."
- "After reviewing the options, I'm going with..."
- "Based on the test results, the decision is..."
- "I'm approving this approach with these conditions..."

**❌ Incorrect:**
- "What should I do?" (That's asking Claude to decide)
- "Which option is best?" (Same problem)
- "Tell me what to choose" (Claude can't do this)

---

## Examples

### ✅ Correct Interaction

**Human:** "I need to choose a frontend framework. Here are my constraints..."

**Claude:** "Here are three hypotheses to consider:
1. **React** (Conservative) - Proven ecosystem, largest talent pool
2. **Vue.js** (Novel) - Better DX, simpler learning curve
3. **Svelte** (Radical) - Best performance, compile-time framework

For each, I've analyzed assumptions, pros, cons, risks..."

**Human:** "I'm rejecting Svelte because we need enterprise support. Between React and Vue.js, I'm choosing React because our team already knows it and the talent pool is larger."

---

### ❌ Incorrect Interaction

**Human:** "Which frontend framework should I use?"

**Claude:** "You should use React because it has the largest ecosystem and talent pool."

**Problem:** Claude is making the decision. This violates the Transformer Mandate.

**Correct Response:** "I can't make that decision for you, but I can generate options for you to consider. Here are three hypotheses..."

---

## In the ADI Cycle

The Transformer Mandate shapes each phase:

### Q1: Abduction (Claude Generates)

```
Claude: "Here are 3-5 hypotheses with rationale,
        assumptions, pros, cons, and risks."
```

### Q2: Deduction (Human Verifies)

```
Human: "I'm verifying each hypothesis. I'll mark them
       PASS or REJECT based on logic, assumptions,
       and constraint compliance."
```

### Q3: Induction (Both Design)

```
Claude: "I've designed validation tests for each
        surviving hypothesis with success criteria."

Human: "I'm reviewing and approving these test plans."
```

### Decision (Human Chooses)

```
Human: "Based on the test results, I'm choosing
       Hypothesis A. Here's my rationale..."
```

---

## Common Pitfalls

### ❌ Don't:
- Ask Claude "what should I do?"
- Expect Claude to make decisions for you
- Treat Claude's suggestions as commands
- Blame Claude for bad decisions (you had the final say)
- Skip the human verification step

### ✅ Do:
- Ask Claude to "generate options"
- Ask Claude to "analyze trade-offs"
- Ask Claude to "design tests"
- Take responsibility for decisions
- Verify Claude's output
- Hold yourself accountable

---

## Why This Matters

### Without the Mandate

1. **AI decides**
   - No verification for hallucinations
   - No accountability for bad outcomes
   - No skin in the game
   - Dangerous for high-stakes decisions

2. **Human becomes passive**
   - Relies on AI for judgment
   - Doesn't develop decision-making skills
   - Can't explain rationale
   - Blames AI when things go wrong

### With the Mandate

1. **AI generates**
   - Provides diverse options
   - Identifies trade-offs
   - Designs validation tests
   - Does what it's good at (generating)

2. **Human decides**
   - Verifies AI output
   - Takes responsibility
   - Learns from process
   - Can explain rationale
   - Owns outcomes

---

## Related Principles

- **PR-0002:** ADI Cycle - The Transformer Mandate shapes the entire cycle
- **Core principle:** All Blackbox3 workflows follow this pattern

---

## Enforcement

The Transformer Mandate is enforced through:

1. **Prompt design:** All FP prompts explicitly avoid decision-making language
2. **Training:** Humans are trained to make the final choice
3. **Documentation:** Decision Records require human signature
4. **Review:** All decisions are reviewed by humans

---

## Examples in Codebase

See:
- `prompts/fp/q1_abduct.md` - Claude generates, doesn't decide
- `prompts/fp/q2_deduct.md` - Human verifies, marks PASS/REJECT
- `prompts/fp/q3_induct.md` - Both design, human approves
- `.docs/first-principles/README.md` - Decision Record template requires human decision maker

---

**Last Updated:** 2026-01-13
**Author:** Blackbox3 First Principles System
