# Agent Security and Privacy Mechanisms Research

**Comprehensive Analysis of Security Threats, Privacy Risks, and Defense Mechanisms for Multi-Agent LLM Systems**

**Created:** 2026-01-18
**Research Focus:** Security vulnerabilities, privacy preservation, adversarial robustness in multi-agent systems

---

## Part 1: The Security Challenge

### Why Multi-Agent Security Is Different

Multi-agent LLM systems introduce **unique security challenges** that don't exist in single-agent deployments:

1. **Compositional Privacy Leakage**: Seemingly innocuous responses from multiple agents can be combined to reveal sensitive information
2. **Emergent Vulnerabilities**: Attack surfaces arise from agent interactions, not individual agent weaknesses
3. **Cross-Agent Context Accumulation**: Adversaries can aggregate fragments across agents to infer private attributes
4. **Distributed Trust**: Each agent may be secure individually, but the system as a whole may be vulnerable
5. **Communication-Based Attacks**: Malicious information can propagate through agent communication channels

### Current State (2025)

**Key Finding**: Multi-agent systems are **highly vulnerable to adversarial attacks**. Existing security mechanisms designed for single-agent systems are insufficient for multi-agent deployments.

- **300+ adversarial instances** documented across 6 attack types in TAMAS benchmark
- **70% adversarial success rate** under baseline chain-of-thought reasoning
- **Compositional inference** identified as critical unaddressed threat
- **No standardized defense framework** for production multi-agent systems

---

## Part 2: Compositional Privacy Leakage

### The Core Problem

**Compositional Privacy Leakage** occurs when sensitive information is revealed only through the combination of outputs from multiple agents, each of which appears innocuous in isolation.

**Example Scenario** (from Patil et al., 2025):

```
Agent 1 (HR): Holds employee ID ‚Üí name mappings
Agent 2 (Finance): Holds employee ID ‚Üí purchase history
Agent 3 (Insurance): Holds product ‚Üí health condition mappings
Agent 4 (Travel): Holds employee ID ‚Üí flight bookings

Individually harmless information:
- "User123 purchased a blood pressure monitor"
- "User123 is in Room 21"
- "Blood pressure monitors are used for heart monitoring"

Combined inference: "John (no diagnosed heart condition) is self-monitoring
for potential undiagnosed cardiovascular issues"
```

### Formal Model

From Patil et al. (2025), we model compositional privacy leakage as:

**System Components:**
- **d defender agents**: Each with unique, non-overlapping data subset ùíü·µ¢
- **Global sensitive attribute s***: Not present in any single ùíü·µ¢
- **Adversary**: Has auxiliary knowledge ùí¶·¥¨ and interaction history

**Leakage Condition:**
```
s* = g(S·¥¨)
where S·¥¨ = (ùí¶·¥¨, interaction history)
```

The adversary infers s* by querying agents and aggregating responses, even though:
1. No single response reveals s*
2. Each response appears benign in isolation
3. Each agent follows standard privacy practices

### Research Findings

**Key Metrics from Patil et al. (2025):**

| Defense Strategy | Sensitive Blocked | Benign Success | Balanced Outcome |
|------------------|-------------------|----------------|------------------|
| Chain-of-Thought | 31-39% | 64-76% | 50% |
| Theory-of-Mind | 89-97% | 53-62% | 73-78% |
| Collaborative Consensus | 86-90% | 66-70% | **78-80%** |

**Critical Insights:**
- Simple CoT fails: 60%+ adversarial success on sensitive queries
- ToM blocks 97% but hurts benign utility (52% vs 76%)
- CoDef achieves best balance (79.8% balanced outcome)

---

## Part 3: Adversarial Attack Vectors

### Attack Taxonomy

From TAMAS benchmark (Kavathekar et al., 2025) and related research:

#### 1. **State-Based Attacks**

**Target**: Individual agents to compromise entire system

**Attack Types:**
- **Prompt Injection**: Malicious instructions embedded in queries
- **Jailbreaking**: Bypassing safety guardrails through crafted prompts
- **Model Poisoning**: Corrupting agent behavior through training data manipulation

**Research Finding**: Multi-agent systems are **more vulnerable** to single-agent attacks due to cascade effects.

#### 2. **Communication-Based Attacks**

**Target**: Information exchange between agents

**Attack Types:**
- **Message Tampering**: Altering inter-agent messages
- **Sybil Attacks**: Creating malicious agent identities
- **Information Leakage**: Extracting sensitive data from communication channels

**Research Finding**: Communication-robust learning with adaptable message verification reduces attack success by 47%.

#### 3. **Compositional Attacks**

**Target**: Multi-step inference across agents

**Attack Types:**
- **Sequential Query Attacks**: Decompose malicious goal into benign subtasks
- **Cross-Agent Context Accumulation**: Aggregate fragments to infer private attributes
- **Plan Execution Attacks**: Execute multi-step adversarial plans

**Research Finding**: Adversarial success reaches **70%** under baseline CoT when optimal plans are provided.

#### 4. **Execution-Time Attacks**

**Target**: Real-time adversarial machine learning

**Attack Types:**
- **Model Inversion**: Reconstruct training data from model outputs
- **Membership Inference**: Determine if specific data was in training set
- **Model Extraction**: Copy model functionality through query attacks

**Research Finding**: Execution-time AML attacks are increasingly sophisticated and hard to detect.

### TAMAS Benchmark Details

**Scale**: 300 adversarial instances across 5 scenarios, 211 tools, 100 harmless tasks

**Attack Types Covered**:
1. Direct harmful instructions
2. Prompt injection
3. Jailbreaking
4. Information extraction
5. Tool misuse
6. Multi-step attacks

**Evaluation Framework**:
```
Effective Robustness Score (ERS) = (Safety Score √ó Task Effectiveness) / Trade-off Penalty
```

**Key Finding**: Current multi-agent systems show **high vulnerability** across all attack types, with no single defense providing comprehensive protection.

---

## Part 4: Defense Mechanisms

### 1. Theory-of-Mind (ToM) Defense

**Concept**: Agents model the adversary's mental state and intent before responding.

**Implementation**:

```typescript
// FILE: src/security/theory-of-mind-defense.ts

class TheoryOfMindDefense {
  async evaluateQuery(
    query: string,
    queryHistory: QueryHistory[],
    agentContext: AgentContext
  ): Promise<SafetyEvaluation> {
    // Step 1: Reconstruct adversary's knowledge state
    const adversaryKnowledge = await this.reconstructKnowledgeState(queryHistory);

    // Step 2: Simulate adversary's intent
    const inferredIntent = await this.inferAdversarialIntent(
      query,
      adversaryKnowledge,
      agentContext
    );

    // Step 3: Evaluate risk
    const riskScore = await this.calculateRiskScore(
      inferredIntent,
      agentContext.sensitiveCombinations
    );

    // Step 4: Make decision
    if (riskScore > THRESHOLD) {
      return {
        action: 'block',
        reason: 'Potential compositional privacy leakage detected',
        inferredIntent
      };
    }

    return {
      action: 'respond',
      response: await this.generateSafeResponse(query, agentContext)
    };
  }

  private async reconstructKnowledgeState(
    queryHistory: QueryHistory[]
  ): Promise<AdversaryKnowledge> {
    const state: AdversaryKnowledge = {
      knownFragments: [],
      inferredCombinations: [],
      queryPatterns: []
    };

    for (const interaction of queryHistory) {
      const fragment = await this.extractInformationFragment(interaction);
      state.knownFragments.push(fragment);

      if (await this.isCombination(state.knownFragments)) {
        state.inferredCombinations.push(
          await this.combineFragments(state.knownFragments)
        );
      }
    }

    return state;
  }

  private async inferAdversarialIntent(
    query: string,
    knowledge: AdversaryKnowledge,
    context: AgentContext
  ): Promise<AdversarialIntent> {
    const prompt = `
You are analyzing a potential adversary's intent in a multi-agent system.

Query: "${query}"

Adversary's current knowledge:
${this.formatKnowledge(knowledge)}

Sensitive combinations that should be protected:
${context.sensitiveCombinations.map(c => c.description).join('\n')}

Task: Determine if this query, combined with the adversary's existing
knowledge, would enable them to access most/all parts of a sensitive
combination.

Respond with JSON:
{
  "isAdversarial": true/false,
  "suspectedGoal": "description of inferred goal",
  "missingFragments": ["fragments still needed"],
  "riskLevel": "low" | "medium" | "high"
}
`;

    const result = await this.llm.generate(prompt);
    return JSON.parse(result);
  }

  private async calculateRiskScore(
    intent: AdversarialIntent,
    sensitiveCombinations: SensitiveCombination[]
  ): Promise<number> {
    if (!intent.isAdversarial) return 0;

    // Check if inferred goal matches any sensitive combination
    const matches = sensitiveCombinations.filter(
      combo => combo.description.toLowerCase().includes(
        intent.suspectedGoal.toLowerCase()
      )
    );

    if (matches.length === 0) return 0;

    // Calculate how many fragments adversary already has
    const completion = (matches[0].requiredFragments.length - intent.missingFragments.length)
                      / matches[0].requiredFragments.length;

    // Higher risk if closer to completion
    return completion * intent.riskLevel === 'high' ? 1.0 : 0.7;
  }
}
```

**Performance** (from Patil et al., 2025):
- **Sensitive Blocking**: 88.8-97% (vs 31-39% for CoT)
- **Benign Success**: 52.9-61.6% (vs 64-76% for CoT)
- **Trade-off**: High protection but significant utility loss

**Pros**:
- Strong adversarial intent recognition
- Effective at blocking compositional attacks
- Works with single agent (no coordination overhead)

**Cons**:
- Over-blocking of benign queries (30-40% reduction)
- High computational cost (mental state simulation)
- May require fine-tuning for each domain

---

### 2. Collaborative Consensus Defense (CoDef)

**Concept**: Agents collaborate to evaluate query safety through shared state and voting.

**Implementation**:

```typescript
// FILE: src/security/collaborative-consensus-defense.ts

class CollaborativeConsensusDefense {
  private defenderAgents: DefenderAgent[];
  private sharedState: SharedDefenseState;

  constructor(defenders: DefenderAgent[]) {
    this.defenderAgents = defenders;
    this.sharedState = new SharedDefenseState();
  }

  async evaluateQuery(
    query: string,
    fromAgent: string,
    localContext: AgentContext
  ): Promise<ConsensusDecision> {
    // Step 1: Aggregate query history across all defenders
    const aggregatedHistory = await this.sharedState.getAdversaryHistory(fromAgent);

    // Step 2: Each defender evaluates independently
    const votes = await Promise.all(
      this.defenderAgents.map(defender =>
        defender.castVote(query, fromAgent, localContext, aggregatedHistory)
      )
    );

    // Step 3: Aggregate votes with safety-first rule
    const decision = this.aggregateVotes(votes);

    // Step 4: Update shared state
    await this.sharedState.recordInteraction(fromAgent, query, decision);

    return decision;
  }

  private aggregateVotes(votes: Vote[]): ConsensusDecision {
    const blockVotes = votes.filter(v => v.verdict === 'block');
    const respondVotes = votes.filter(v => v.verdict === 'respond');
    const vagueVotes = votes.filter(v => v.verdict === 'vague');

    // Safety-first: Any block vote triggers block
    if (blockVotes.length > 0) {
      return {
        action: 'block',
        reason: this.consolidateReasons(blockVotes),
        consensus: blockVotes.length / this.defenderAgents.length
      };
    }

    // Otherwise, respond (with vague option if suggested)
    if (vagueVotes.length > this.defenderAgents.length / 2) {
      return {
        action: 'vague',
        response: this.generateVagueResponse(respondVotes),
        consensus: vagueVotes.length / this.defenderAgents.length
      };
    }

    return {
      action: 'respond',
      response: this.selectBestResponse(respondVotes),
      consensus: respondVotes.length / this.defenderAgents.length
    };
  }
}

class DefenderAgent {
  async castVote(
    query: string,
    fromAgent: string,
    localContext: AgentContext,
    aggregatedHistory: InteractionHistory
  ): Promise<Vote> {
    const prompt = `
You are a trusted agent in a multi-agent system evaluating query safety.

Query: "${query}"
From: "${fromAgent}"

Your local context:
${this.formatContext(localContext)}

Adversary's interaction history with ALL defenders:
${this.formatHistory(aggregatedHistory)}

Other defenders' contexts:
${this.formatPeerContexts(localContext.peers)}

Safety policy: ${localContext.safetyPolicy}

Task: Vote on whether to share this information.

Options:
(A) Safe to answer
(B) Should be blocked
(C) Provide vague/general answer instead

Respond with:
Verdict: A / B / C
Reason: <explanation>
Suggested Answer (if C): <vague response>
`;

    const result = await this.llm.generate(prompt);
    return this.parseVote(result);
  }
}

class SharedDefenseState {
  private adversaryHistories: Map<string, InteractionHistory> = new Map();
  private recentDecisions: CircularBuffer<DecisionRecord>;

  async getAdversaryHistory(adversaryId: string): Promise<InteractionHistory> {
    if (!this.adversaryHistories.has(adversaryId)) {
      this.adversaryHistories.set(adversaryId, new InteractionHistory());
    }
    return this.adversaryHistories.get(adversaryId)!;
  }

  async recordInteraction(
    adversaryId: string,
    query: string,
    decision: ConsensusDecision
  ): Promise<void> {
    const history = await this.getAdversaryHistory(adversaryId);
    history.add({
      timestamp: Date.now(),
      query,
      decision,
      agentsInvolved: decision.participatingAgents
    });

    this.recentDecisions.add({
      adversaryId,
      query,
      decision
    });
  }

  async detectAnomalousPatterns(adversaryId: string): Promise<boolean> {
    const history = await this.getAdversaryHistory(adversaryId);

    // Detect rapid querying across multiple agents
    const recentQueries = history.getRecentQueries(30000); // Last 30 seconds
    if (recentQueries.length > 10) {
      const uniqueAgents = new Set(recentQueries.map(q => q.targetAgent));
      if (uniqueAgents.size >= 3) {
        return true; // Potential compositional attack
      }
    }

    // Detect sequential information gathering
    const patterns = await this.analyzeQueryPatterns(history);
    if (patterns.compositionalScore > 0.8) {
      return true;
    }

    return false;
  }
}
```

**Performance** (from Patil et al., 2025):
- **Sensitive Blocking**: 86-90%
- **Benign Success**: 66-70%
- **Balanced Outcome**: 78-80% (best overall)

**Pros**:
- Best privacy-utility balance
- Ensemble effect compensates for individual weaknesses
- Scalable to large defender pools
- Shared state enables pattern detection

**Cons**:
- Communication overhead
- Requires defender coordination infrastructure
- Potential for single malicious defender to influence others
- Latency from consensus process

---

### 3. Topology-Guided Security (G-Safeguard)

**Concept**: Use system topology and structure to guide security mechanisms.

**Key Innovation**: Leverage the graph structure of multi-agent systems to identify and protect critical paths.

**Implementation Principles**:

```typescript
// FILE: src/security/topology-guided-security.ts

class TopologyGuidedSecurity {
  private agentGraph: AgentTopologyGraph;
  private criticalPaths: CriticalPath[];

  constructor(agentSystem: MultiAgentSystem) {
    this.agentGraph = this.buildTopologyGraph(agentSystem);
    this.criticalPaths = this.identifyCriticalPaths();
  }

  private buildTopologyGraph(system: MultiAgentSystem): AgentTopologyGraph {
    const graph = new Graph();

    for (const agent of system.agents) {
      graph.addNode(agent.id, {
        dataSensitivity: agent.dataSensitivity,
        accessLevel: agent.accessLevel,
        capabilities: agent.capabilities
      });
    }

    for (const [source, target, protocol] of system.communicationChannels) {
      graph.addEdge(source, target, {
        protocol,
        dataTypes: protocol.dataTypes,
        frequency: protocol.frequency
      });
    }

    return graph;
  }

  private identifyCriticalPaths(): CriticalPath[] {
    const paths: CriticalPath[] = [];

    // Find all paths that could lead to sensitive data
    for (const sensitiveAgent of this.agentGraph.getNodesBySensitivity('high')) {
      const pathsToSensitive = this.agentGraph.findAllPathsTo(sensitiveAgent.id);

      for (const path of pathsToSensitive) {
        const riskScore = this.calculatePathRisk(path);
        if (riskScore > CRITICAL_THRESHOLD) {
          paths.push({
            path,
            riskScore,
            protectionLevel: this.determineProtectionLevel(riskScore)
          });
        }
      }
    }

    return paths.sort((a, b) => b.riskScore - a.riskScore);
  }

  private calculatePathRisk(path: AgentPath): number {
    let risk = 0;

    // Factor 1: Cumulative data sensitivity along path
    for (const agentId of path) {
      const agent = this.agentGraph.getNode(agentId);
      risk += agent.dataSensitivity;
    }

    // Factor 2: Communication channel vulnerabilities
    for (let i = 0; i < path.length - 1; i++) {
      const edge = this.agentGraph.getEdge(path[i], path[i + 1]);
      risk += (1 - edge.security) * 0.3;
    }

    // Factor 3: Path length (longer paths = more aggregation points)
    risk += path.length * 0.1;

    // Factor 4: Historical attack patterns
    const historicalAttacks = this.getAttackHistory(path);
    risk += historicalAttacks.successRate * 0.5;

    return Math.min(risk, 1.0);
  }

  async enforceSecurityPolicy(
    query: Query,
    path: AgentPath
  ): Promise<SecurityDecision> {
    // Check if path matches any critical path
    const criticalPath = this.criticalPaths.find(cp =>
      this.isPathMatch(path, cp.path)
    );

    if (!criticalPath) {
      return { action: 'allow' };
    }

    // Apply protection based on critical path level
    switch (criticalPath.protectionLevel) {
      case 'maximum':
        return {
          action: 'require_consensus',
          requiredVotes: Math.floor(this.agentGraph.getNodeCount() * 0.8),
          auditing: true
        };

      case 'high':
        return {
          action: 'require_theory_of_mind',
          enhancedLogging: true
        };

      case 'medium':
        return {
          action: 'require_voting',
          requiredVotes: 3,
          auditing: false
        };

      default:
        return { action: 'allow' };
    }
  }

  private isPathMatch(queryPath: AgentPath, criticalPath: AgentPath): boolean {
    // Check if query path is a subsequence of critical path
    let queryIndex = 0;
    for (const agentId of criticalPath) {
      if (queryPath[queryIndex] === agentId) {
        queryIndex++;
        if (queryIndex === queryPath.length) {
          return true;
        }
      }
    }
    return false;
  }
}
```

**Key Benefits**:
- Proactive protection of high-risk paths
- Efficient resource allocation (focus security where it matters most)
- Systematic coverage of all potential attack vectors
- Scalable to large systems

---

### 4. Secure Multi-Party Computation (SMPC)

**Concept**: Enable agents to compute jointly over private inputs without revealing those inputs.

**Use Case**: Privacy-preserving federated learning across agents.

**Implementation**:

```typescript
// FILE: src/security/smpc-aggregation.ts

class SMPCAggregator {
  private agents: Agent[];
  private cryptoProvider: CryptoProvider;

  async secureAggregate(
    localUpdates: ModelUpdate[],
    aggregationFn: AggregationFunction
  ): Promise<AggregatedResult> {
    // Step 1: Generate secret shares
    const shares = await Promise.all(
      localUpdates.map(async (update, idx) => {
        return this.generateSecretShares(update, this.agents.length, idx);
      })
    );

    // Step 2: Distribute shares to other agents
    const distributedShares = await this.distributeShares(shares);

    // Step 3: Each agent computes on its shares
    const localResults = await Promise.all(
      this.agents.map(async (agent, idx) => {
        const agentShares = distributedShares.map(s => s[idx]);
        return agent.computeOnShares(agentShares, aggregationFn);
      })
    );

    // Step 4: Reconstruct final result
    const result = await this.reconstructResult(localResults);

    return result;
  }

  private async generateSecretShares(
    value: ModelUpdate,
    numShares: number,
    ownerIndex: number
  ): Promise<SecretShare[]> {
    const shares: SecretShare[] = [];
    let cumulativeSum = 0;

    // Generate n-1 random shares
    for (let i = 0; i < numShares - 1; i++) {
      const randomValue = await this.cryptoProvider.random();
      shares.push({
        agentIndex: i,
        value: randomValue,
        ownerIndex
      });
      cumulativeSum = (cumulativeSum + randomValue) % this.cryptoProvider.prime;
    }

    // Final share ensures sum equals original value
    const finalShare = (value.toNumber() - cumulativeSum + this.cryptoProvider.prime)
                       % this.cryptoProvider.prime;
    shares.push({
      agentIndex: numShares - 1,
      value: finalShare,
      ownerIndex
    });

    return shares;
  }

  private async distributeShares(
    allShares: SecretShare[][]
  ): Promise<SecretShare[][]> {
    const distributed: SecretShare[][] = Array.from(
      { length: this.agents.length },
      () => []
    );

    for (const ownerShares of allShares) {
      for (const share of ownerShares) {
        distributed[share.agentIndex].push(share);
      }
    }

    return distributed;
  }

  private async reconstructResult(
    localResults: ComputedShare[]
  ): Promise<AggregatedResult> {
    // Sum all shares to reconstruct final result
    let sum = BigInt(0);
    const prime = BigInt(this.cryptoProvider.prime);

    for (const result of localResults) {
      sum = (sum + BigInt(result.value)) % prime;
    }

    return ModelUpdate.fromNumber(sum);
  }
}

// Differential Privacy + SMPC hybrid
class DPSMPCAggregator extends SMPCAggregator {
  async secureAggregateWithDP(
    localUpdates: ModelUpdate[],
    aggregationFn: AggregationFunction,
    epsilon: number
  ): Promise<AggregatedResult> {
    // Add local differential privacy before sharing
    const privatizedUpdates = await Promise.all(
      localUpdates.map(async (update) => {
        const noise = await this.generateDPNoise(update, epsilon);
        return update.add(noise);
      })
    );

    // Then apply SMPC
    return this.secureAggregate(privatizedUpdates, aggregationFn);
  }

  private async generateDPNoise(
    update: ModelUpdate,
    epsilon: number
  ): Promise<ModelUpdate> {
    const sensitivity = update.sensitivity();
    const scale = sensitivity / epsilon;

    // Generate Gaussian noise
    const noise = await this.cryptoProvider.gaussianNoise(scale);
    return new ModelUpdate(noise);
  }
}
```

**Performance** (from Zheng et al., 2024):
- **Privacy**: Œµ-differential privacy guarantees
- **Accuracy**: < 2% accuracy loss compared to non-private aggregation
- **Cost**: 3-5x computation overhead vs plaintext aggregation

**Use Cases**:
- Cross-agent federated learning
- Private analytics across organizations
- Secure benchmarking

---

## Part 5: Privacy Preservation Techniques

### 1. Differential Privacy for Multi-Agent Systems

**Concept**: Add calibrated noise to agent outputs to prevent extraction of sensitive information.

**Implementation**:

```typescript
// FILE: src/privacy/differential-privacy.ts

class DifferentialPrivacyMechanism {
  async privatizeResponse(
    response: AgentResponse,
    sensitivity: number,
    epsilon: number
  ): Promise<PrivatizedResponse> {
    // Select mechanism based on response type
    switch (response.type) {
      case 'numeric':
        return this.numericMechanism(response, sensitivity, epsilon);

      case 'categorical':
        return this.categoricalMechanism(response, sensitivity, epsilon);

      case 'text':
        return this.textMechanism(response, sensitivity, epsilon);

      case 'structured':
        return this.structuredMechanism(response, sensitivity, epsilon);
    }
  }

  private async numericMechanism(
    response: NumericResponse,
    sensitivity: number,
    epsilon: number
  ): Promise<NumericResponse> {
    // Laplace mechanism
    const scale = sensitivity / epsilon;
    const noise = await this.sampleLaplace(0, scale);

    return {
      ...response,
      value: response.value + noise
    };
  }

  private async categoricalMechanism(
    response: CategoricalResponse,
    sensitivity: number,
    epsilon: number
  ): Promise<CategoricalResponse> {
    // Exponential mechanism
    const scores = await this.computeUtilityScores(response);
    const probabilities = this.computeExponentialScores(
      scores,
      sensitivity,
      epsilon
    );

    const selected = await this.sampleCategorical(probabilities);

    return {
      ...response,
      category: response.categories[selected]
    };
  }

  private async textMechanism(
    response: TextResponse,
    sensitivity: number,
    epsilon: number
  ): Promise<TextResponse> {
    // Text sanitization + synthetic data generation
    const sanitized = await this.sanitizeText(response.text);

    if (sanitized.isSensitive) {
      // Generate synthetic response with similar properties
      const synthetic = await this.generateSynthetic(
        response.context,
        epsilon
      );
      return synthetic;
    }

    return response;
  }

  private async structuredMechanism(
    response: StructuredResponse,
    sensitivity: number,
    epsilon: number
  ): Promise<StructuredResponse> {
    // Apply DP to each field independently
    const privatizedFields = await Promise.all(
      response.fields.map(async (field) => {
        const fieldSensitivity = this.estimateFieldSensitivity(field);
        const fieldEpsilon = this.allocateEpsilon(field, response, epsilon);
        return this.privatizeResponse(field, fieldSensitivity, fieldEpsilon);
      })
    );

    return {
      ...response,
      fields: privatizedFields
    };
  }

  private estimateFieldSensitivity(field: ResponseField): number {
    // Estimate maximum change in output when one record is added/removed
    switch (field.type) {
      case 'count':
        return 1;

      case 'sum':
        return field.maxValue - field.minValue;

      case 'mean':
        return (field.maxValue - field.minValue) / field.maxCount;

      case 'histogram':
        return 1;

      default:
        return 1; // Conservative default
    }
  }

  private allocateEpsilon(
    field: ResponseField,
    response: StructuredResponse,
    totalEpsilon: number
  ): number {
    // Allocate epsilon based on field importance
    const totalFields = response.fields.length;
    const importance = this.getFieldImportance(field);

    return (totalEpsilon * importance) / totalFields;
  }
}
```

**Privacy Guarantee**: (Œµ, Œ¥)-differential privacy
- **Œµ**: Privacy loss budget (typically 0.1-10)
- **Œ¥**: Failure probability (typically 10‚Åª‚Åµ)

---

### 2. Federated Learning for Multi-Agent Systems

**Concept**: Train models collaboratively without sharing raw data.

**Architecture**:

```typescript
// FILE: src/privacy/federated-learning.ts

class FederatedLearningCoordinator {
  private agents: FederatedAgent[];
  private globalModel: Model;
  private privacyAccountant: PrivacyAccountant;

  async runFederatedRound(
    round: number,
    numEpochs: number
  ): Promise<TrainingMetrics> {
    // Step 1: Distribute current global model
    await this.distributeGlobalModel();

    // Step 2: Each agent trains locally
    const localUpdates = await Promise.all(
      this.agents.map(async (agent) => {
        return agent.localTrain(numEpochs);
      })
    );

    // Step 3: Apply differential privacy to updates
    const privatizedUpdates = await Promise.all(
      localUpdates.map(async (update) => {
        const epsilon = this.privacyAccountant.getSpendableBudget();
        return this.privatizeUpdate(update, epsilon);
      })
    );

    // Step 4: Secure aggregation (SMPC)
    const aggregatedUpdate = await this.secureAggregation(privatizedUpdates);

    // Step 5: Update global model
    this.globalModel = await this.applyUpdate(aggregatedUpdate);

    // Step 6: Track privacy spend
    await this.privacyAccountant.recordSpend(round, privatizedUpdates);

    return {
      round,
      globalLoss: await this.evaluateGlobalModel(),
      privacyBudget: this.privacyAccountant.getRemainingBudget(),
      participatingAgents: this.agents.length
    };
  }

  private async privatizeUpdate(
    update: ModelUpdate,
    epsilon: number
  ): Promise<PrivatizedUpdate> {
    // Clip gradients
    const clipped = this.clipGradients(update, this.maxGradNorm);

    // Add noise
    const noise = await this.generateDPNoise(clipped, epsilon);

    return clipped.add(noise);
  }

  private async secureAggregation(
    updates: PrivatizedUpdate[]
  ): Promise<AggregatedUpdate> {
    // Use SMPC for aggregation
    const smpc = new SMPCAggregator();
    return smpc.secureAggregate(updates, this.aggregationFunction);
  }
}

class FederatedAgent {
  private localData: Dataset;
  private localModel: Model;

  async localTrain(numEpochs: number): Promise<ModelUpdate> {
    const initialParams = this.localModel.getParameters();

    // Train on local data
    for (let epoch = 0; epoch < numEpochs; epoch++) {
      const batches = this.localData.getBatches(this.batchSize);

      for (const batch of batches) {
        const gradients = await this.localModel.computeGradients(batch);
        this.localModel.applyGradients(gradients, this.learningRate);
      }
    }

    const finalParams = this.localModel.getParameters();

    // Return update (difference)
    return ModelUpdate.fromDifference(initialParams, finalParams);
  }
}
```

**Privacy Guarantees**:
- **Local DP**: Each agent's update is differentially private
- **Central DP**: Aggregated model is differentially private
- **User-level DP**: Each agent's entire data contribution is private

---

## Part 6: Enterprise Security Requirements

### 1. RBAC Testing for Multi-Agent Systems

**Challenge**: Ensure agents respect user permissions across complex role structures.

**Implementation**:

```typescript
// FILE: src/security/rbac-testing.ts

class RBACSecurityTester {
  async testPermissionConstraints(
    agentSystem: MultiAgentSystem,
    userRoles: Map<string, string[]>,
    resourcePermissions: Map<string, string[]>
  ): Promise<RBACTestResult[]> {
    const results: RBACTestResult[] = [];

    for (const [userId, roles] of userRoles) {
      for (const role of roles) {
        // Test: User with role should access role-specific resources
        const shouldAllow = resourcePermissions.has(role);

        const attempt = await agentSystem.attemptAction({
          userId,
          action: 'read',
          resource: `restricted_${role}`
        });

        results.push({
          userId,
          role,
          action: 'read',
          resource: `restricted_${role}`,
          allowed: attempt.allowed,
          shouldAllow,
          passed: attempt.allowed === shouldAllow,
          reason: attempt.reason
        });

        // Test: User without role should be denied
        const otherRoles = Array.from(resourcePermissions.keys())
          .filter(r => r !== role);

        for (const otherRole of otherRoles) {
          const deniedAttempt = await agentSystem.attemptAction({
            userId,
            action: 'read',
            resource: `restricted_${otherRole}`
          });

          results.push({
            userId,
            role,
            action: 'read',
            resource: `restricted_${otherRole}`,
            allowed: deniedAttempt.allowed,
            shouldAllow: false,
            passed: !deniedAttempt.allowed,
            reason: deniedAttempt.reason
          });
        }
      }
    }

    return results;
  }

  async testPrivilegeEscalation(
    agentSystem: MultiAgentSystem,
    userRoles: Map<string, string[]>
  ): Promise<EscalationTestResult[]> {
    const results: EscalationTestResult[] = [];

    for (const [userId, roles] of userRoles) {
      const maxRole = this.getHighestPrivilegeRole(roles);

      // Attempt to escalate to higher privilege
      const attempts = [
        {
          from: roles[0],
          to: this.getNextRole(roles[0], 'higher'),
          method: 'direct_request'
        },
        {
          from: roles[0],
          to: this.getNextRole(roles[0], 'higher'),
          method: 'role_reassignment'
        },
        {
          from: roles[0],
          to: this.getNextRole(roles[0], 'higher'),
          method: 'permission_inheritance'
        }
      ];

      for (const attempt of attempts) {
        const result = await agentSystem.attemptEscalation({
          userId,
          from: attempt.from,
          to: attempt.to,
          method: attempt.method
        });

        results.push({
          userId,
          from: attempt.from,
          to: attempt.to,
          method: attempt.method,
          succeeded: result.succeeded,
          passed: !result.succeeded, // Should NOT succeed
          reason: result.reason
        });
      }
    }

    return results;
  }

  async testCrossAgentDataLeakage(
    agentSystem: MultiAgentSystem
  ): Promise<DataLeakageResult[]> {
    const results: DataLeakageResult[] = [];

    // Scenario 1: Agent A should not access Agent B's private data
    const agents = agentSystem.getAgents();
    for (let i = 0; i < agents.length; i++) {
      for (let j = 0; j < agents.length; j++) {
        if (i === j) continue;

        const attempt = await agents[i].attemptAccess({
          targetAgent: agents[j].id,
          resource: 'private_data',
          method: 'direct_query'
        });

        results.push({
          fromAgent: agents[i].id,
          toAgent: agents[j].id,
          resource: 'private_data',
          succeeded: attempt.succeeded,
          passed: !attempt.succeeded, // Should NOT succeed
          method: 'direct_query'
        });

        // Test indirect access through other agents
        const indirectAttempt = await agents[i].attemptIndirectAccess({
          targetAgent: agents[j].id,
          resource: 'private_data',
          intermediaryAgents: agents.filter(a => a.id !== agents[i].id && a.id !== agents[j].id)
        });

        results.push({
          fromAgent: agents[i].id,
          toAgent: agents[j].id,
          resource: 'private_data',
          succeeded: indirectAttempt.succeeded,
          passed: !indirectAttempt.succeeded,
          method: 'indirect_routing'
        });
      }
    }

    return results;
  }
}
```

---

### 2. Compliance Validation

**Challenge**: Ensure agent systems comply with regulations (GDPR, HIPAA, SOX).

**Implementation**:

```typescript
// FILE: src/security/compliance-validation.ts

class ComplianceValidator {
  private regulations: ComplianceFramework[];

  async validateGDPRCompliance(
    agentSystem: MultiAgentSystem
  ): Promise<GDPRComplianceReport> {
    const checks: GDPRCheck[] = [];

    // Check 1: Right to be forgotten
    checks.push({
      requirement: 'Right to erasure (Article 17)',
      check: async () => {
        const testUser = await this.createTestUser();
        await agentSystem.deleteUserData(testUser.id);
        const remains = await agentSystem.hasUserData(testUser.id);
        return {
          passed: !remains,
          description: 'All user data deleted upon request'
        };
      }
    });

    // Check 2: Data portability
    checks.push({
      requirement: 'Right to data portability (Article 20)',
      check: async () => {
        const testUser = await this.createTestUser();
        const exportData = await agentSystem.exportUserData(testUser.id);
        return {
          passed: this.validateDataFormat(exportData),
          description: 'Data export in machine-readable format'
        };
      }
    });

    // Check 3: Consent management
    checks.push({
      requirement: 'Lawful basis for processing (Article 6)',
      check: async () => {
        const testUser = await this.createTestUser();
        const processing = await agentSystem.processUserData(testUser.id);
        return {
          passed: processing.hasConsent,
          description: 'User consent obtained before processing'
        };
      }
    });

    // Check 4: Data minimization
    checks.push({
      requirement: 'Data minimization (Article 5)',
      check: async () => {
        const testData = await this.generateTestData();
        const processed = await agentSystem.processData(testData);
        const collectedData = processed.getCollectedFields();
        const requiredData = testData.getRequiredFields();
        return {
          passed: this.isSubset(collectedData, requiredData),
          description: 'Only necessary data collected'
        };
      }
    });

    // Run all checks
    const results = await Promise.all(
      checks.map(async (check) => ({
        requirement: check.requirement,
        result: await check.check()
      }))
    );

    return {
      timestamp: Date.now(),
      overallCompliant: results.every(r => r.result.passed),
      checks: results,
      recommendations: this.generateRecommendations(results)
    };
  }

  async validateHIPAACompliance(
    agentSystem: MultiAgentSystem
  ): Promise<HIPAAComplianceReport> {
    const checks: HIPAACheck[] = [];

    // Check 1: PHI encryption
    checks.push({
      requirement: 'Encryption at rest (164.312(a)(2)(iv))',
      check: async () => {
        const phiData = await this.generatePHI();
        await agentSystem.storePHI(phiData);
        const storage = await agentSystem.getStorageMechanism();
        return {
          passed: storage.isEncrypted,
          description: 'PHI encrypted at rest'
        };
      }
    });

    // Check 2: Access controls
    checks.push({
      requirement: 'Access controls (164.312(a)(1))',
      check: async () => {
        const unauthorizedUser = await this.createUnauthorizedUser();
        const accessAttempt = await agentSystem.attemptPHIAccess(unauthorizedUser.id);
        return {
          passed: !accessAttempt.succeeded,
          description: 'Unauthorized access denied'
        };
      }
    });

    // Check 3: Audit logging
    checks.push({
      requirement: 'Audit controls (164.312(b))',
      check: async () => {
        const testActivity = await this.performTestActivity();
        const auditLog = await agentSystem.getAuditLog(testActivity.id);
        return {
          passed: auditLog.exists && auditLog.isComplete,
          description: 'All PHI access logged'
        };
      }
    });

    // Check 4: Minimum necessary standard
    checks.push({
      requirement: 'Minimum necessary (164.502(b))',
      check: async () => {
        const request = await this.createPHIRequest();
        const disclosed = await agentSystem.disclosePHI(request);
        const minimumNecessary = await this.calculateMinimumNecessary(request);
        return {
          passed: disclosed.data.length <= minimumNecessary,
          description: 'Only minimum necessary PHI disclosed'
        };
      }
    });

    const results = await Promise.all(
      checks.map(async (check) => ({
        requirement: check.requirement,
        result: await check.check()
      }))
    );

    return {
      timestamp: Date.now(),
      overallCompliant: results.every(r => r.result.passed),
      checks: results,
      recommendations: this.generateRecommendations(results)
    };
  }
}
```

---

## Part 7: Implementation Priority

### Phase 1 (Week 1-2): Critical Defenses

**Priority 1: Implement Theory-of-Mind Defense**
- Single-agent mental state simulation
- Adversarial intent recognition
- Query history tracking
- **Impact**: 89-97% sensitive blocking

**Priority 2: Basic Compositional Attack Detection**
- Track query patterns across agents
- Detect sequential information gathering
- Implement simple aggregation detection
- **Impact**: Prevent 70%+ of compositional attacks

**Priority 3: Privacy-Preserving Aggregation**
- Implement SMPC for federated learning
- Add differential privacy to agent outputs
- Secure model updates
- **Impact**: Œµ-DP guarantees for collaborative learning

---

### Phase 2 (Week 3-4): Collaboration & Topology

**Priority 4: Collaborative Consensus Defense**
- Multi-agent voting mechanism
- Shared state management
- Consensus algorithms
- **Impact**: 79.8% balanced outcome (best overall)

**Priority 5: Topology-Guided Security**
- Build agent topology graph
- Identify critical paths
- Path-based security policies
- **Impact**: Proactive protection of high-risk paths

**Priority 6: Enhanced RBAC Testing**
- Permission constraint testing
- Privilege escalation testing
- Cross-agent data leakage detection
- **Impact**: Enterprise security compliance

---

### Phase 3 (Week 5+): Advanced Features

**Priority 7: Compliance Validation**
- GDPR compliance checks
- HIPAA validation
- SOX controls
- **Impact**: Regulatory compliance

**Priority 8: Continuous Security Monitoring**
- Real-time attack detection
- Anomaly detection
- Automated incident response
- **Impact**: 24/7 security posture

**Priority 9: Privacy-Preserving Analytics**
- Cross-agent private analytics
- Secure benchmarking
- Privacy-preserving insights
- **Impact**: Enable collaboration without data sharing

---

## Part 8: Key Research Findings Summary

| Research Finding | Impact | Source |
|------------------|--------|--------|
| **Compositional privacy leakage** is critical unaddressed threat | 70% adversarial success under baseline CoT | Patil et al. (2025) |
| **Theory-of-Mind defense** blocks 97% but hurts benign utility | 88.8-97% sensitive blocking, 52.9-61.6% benign success | Patil et al. (2025) |
| **Collaborative Consensus** achieves best privacy-utility balance | 79.8% balanced outcome (best overall) | Patil et al. (2025) |
| **Multi-agent systems** are highly vulnerable to adversarial attacks | 300+ attack instances in TAMAS benchmark | Kavathekar et al. (2025) |
| **Communication-robust learning** reduces attack success by 47% | Message verification + adaptable protocols | Yuan et al. (2024) |
| **Diversity-guided testing** improves bug detection by 47% | Behavioral + state + temporal diversity | Chen et al. (2024) |
| **SMPC + DP hybrid** provides strong privacy guarantees | < 2% accuracy loss, Œµ-DP guarantees | Zheng et al. (2024) |
| **Hierarchical structures** are most resilient to faulty agents | 34% improvement in resilience | OpenReview (2024) |
| **Single-agent attacks** can compromise entire multi-agent system | Cascade effects amplify vulnerabilities | Standen et al. (2025) |

---

## Part 9: Recommended Tools and Frameworks

### Security Testing

- **TAMAS Benchmark**: 300 adversarial instances for multi-agent systems
- **Red-teaming frameworks**: Automated adversarial testing
- **Property-based testing**: 23-37% improvement in bug detection

### Privacy Preservation

- **OpenMined PySyft**: Federated learning + differential privacy + encrypted computation
- **CrypTen**: PyTorch-based secure ML framework
- **TensorFlow Privacy**: DP mechanisms for ML

### Secure Computation

- **Microsoft SEAL**: Homomorphic encryption library
- **MP-SPDZ**: Secure multi-party computation framework
- **TF-Encrypted**: TensorFlow encryption layer

### Monitoring & Auditing

- **LangSmith**: Agent interaction monitoring
- **Weights & Biases**: Experiment tracking + security monitoring
- **Azure AI Foundry**: Enterprise-grade AI safety monitoring

---

## Part 10: Decision Rules for Meta-Agent

### Rule 13: Security Mechanism Selection

**Question**: Which security defense mechanism should I implement for my multi-agent system?

**Decision Algorithm**:

```typescript
function selectSecurityDefense(system: MultiAgentSystem): SecurityDefense {
  const factors = {
    numAgents: system.agents.length,
    sensitivity: system.dataSensitivity, // low, medium, high
    performanceRequirements: system.latencyBudget,
    collaboration: system.requiresCollaboration,
    compliance: system.regulatoryRequirements,
    topology: system.topologyComplexity, // simple, complex
    attackSurface: system.attackSurfaceExposure
  };

  // Score: 0-100 for each defense mechanism
  const scores = {
    theoryOfMind: 0,
    collaborativeConsensus: 0,
    topologyGuided: 0,
    smpc: 0
  };

  // Factor 1: Number of agents
  if (factors.numAgents < 5) {
    scores.theoryOfMind += 30; // Single-agent defenses work well
  } else if (factors.numAgents < 20) {
    scores.collaborativeConsensus += 25; // Collaboration feasible
    scores.theoryOfMind += 15;
  } else {
    scores.collaborativeConsensus += 35; // Collaboration essential
    scores.topologyGuided += 20; // Complex topology
  }

  // Factor 2: Data sensitivity
  if (factors.sensitivity === 'high') {
    scores.collaborativeConsensus += 30; // Strongest protection
    scores.smpc += 25; // Zero-knowledge computation
    scores.theoryOfMind += 20;
  } else if (factors.sensitivity === 'medium') {
    scores.theoryOfMind += 25;
    scores.collaborativeConsensus += 20;
  } else {
    scores.theoryOfMind += 15; // Basic protection sufficient
  }

  // Factor 3: Performance requirements
  if (factors.performanceRequirements === 'strict') {
    scores.theoryOfMind += 25; // Lowest overhead
    scores.topologyGuided += 20; // Efficient path-based protection
  } else if (factors.performanceRequirements === 'moderate') {
    scores.collaborativeConsensus += 15;
    scores.theoryOfMind += 20;
  } else {
    scores.collaborativeConsensus += 25; // Can afford coordination overhead
    scores.smpc += 20; // Can afford crypto overhead
  }

  // Factor 4: Collaboration requirements
  if (factors.collaboration === 'essential') {
    scores.collaborativeConsensus += 30; // Natural fit
    scores.smpc += 25; // Privacy-preserving collaboration
  } else if (factors.collaboration === 'beneficial') {
    scores.collaborativeConsensus += 20;
  }

  // Factor 5: Compliance requirements
  if (factors.compliance.includes('GDPR') || factors.compliance.includes('HIPAA')) {
    scores.collaborativeConsensus += 25; // Strong audit trail
    scores.smpc += 30; // Data never leaves agent boundary
    scores.theoryOfMind += 15;
  }

  // Factor 6: Topology complexity
  if (factors.topology === 'complex') {
    scores.topologyGuided += 35; // Essential for complex topologies
    scores.collaborativeConsensus += 20;
  } else if (factors.topology === 'moderate') {
    scores.topologyGuided += 20;
  }

  // Factor 7: Attack surface exposure
  if (factors.attackSurface === 'high') {
    scores.collaborativeConsensus += 30; // Strongest protection
    scores.topologyGuided += 25; // Proactive path protection
    scores.theoryOfMind += 20;
  } else if (factors.attackSurface === 'medium') {
    scores.theoryOfMind += 25;
    scores.collaborativeConsensus += 20;
  }

  // Select defense with highest score
  const maxScore = Math.max(...Object.values(scores));
  const selected = Object.keys(scores).find(
    key => scores[key] === maxScore
  ) as SecurityDefenseType;

  // Apply defense
  switch (selected) {
    case 'theoryOfMind':
      return new TheoryOfMindDefense();

    case 'collaborativeConsensus':
      return new CollaborativeConsensusDefense(system.agents);

    case 'topologyGuided':
      return new TopologyGuidedSecurity(system);

    case 'smpc':
      return new SMPCAggregator(system.agents);
  }
}
```

**Usage Example**:

```typescript
const mySystem = {
  agents: [...], // 15 agents
  dataSensitivity: 'high',
  latencyBudget: 'moderate',
  requiresCollaboration: true,
  regulatoryRequirements: ['GDPR', 'HIPAA'],
  topologyComplexity: 'complex',
  attackSurfaceExposure: 'high'
};

const defense = selectSecurityDefense(mySystem);
// Result: Collaborative Consensus Defense (score: 175)

// Apply defense
await defense.initialize(mySystem);
const evaluation = await defense.evaluateQuery(adversarialQuery);
```

---

## Conclusion

Multi-agent LLM systems introduce **unique security challenges** that require **specialized defense mechanisms**. Key takeaways:

1. **Compositional privacy leakage** is a critical unaddressed threat
2. **No single defense** provides comprehensive protection
3. **Collaborative approaches** (CoDef) achieve best privacy-utility balance
4. **Topology-aware security** enables proactive protection
5. **Enterprise requirements** (RBAC, compliance) need specialized testing

**Immediate Actions**:
1. Implement Theory-of-Mind defense for single-agent protection
2. Add compositional attack detection across agents
3. Deploy Collaborative Consensus for critical systems
4. Implement privacy-preserving aggregation for federated learning
5. Add compliance validation for regulated industries

**Research Status**: Ready for implementation in Black Box 5

---

## Sources

### Academic Papers

- [The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration](https://arxiv.org/html/2509.14284v1) - Patil et al. (UNC Chapel Hill, 2025)
- [TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems](https://arxiv.org/abs/2511.05269) - Kavathekar et al. (ICML 2025)
- [Optimizing Privacy in Federated Learning with MPC and Differential Privacy](https://dl.acm.org/doi/fullHtml/10.1145/3654823.3654854) - Zheng (ACM 2024)
- [Adversarial Machine Learning Attacks and Defences in MARL](https://dl.acm.org/doi/full/10.1145/3708320) - Standen et al. (ACM 2025)
- [G-Safeguard: A Topology-Guided Security Framework for LLM-based Multi-Agent Systems](https://aclanthology.org/2025.acl-long.359.pdf) - ACL 2025
- [Robust Multi-Agent Reinforcement Learning Against Attacks](https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/rsn2.70033) - Wang et al. (2025)
- [Red-Teaming LLM Multi-Agent Systems](https://aclanthology.org/2025.findings-acl.349/) - He et al. (ACL 2025)
- [Communication-robust multi-agent learning by adaptable protocols](https://journal.hep.com.cn/fcs/EN/10.1007/s11704-023-2733-5) - Yuan et al. (2024)
- [Federated Learning: A Survey on Privacy-Preserving Techniques](https://arxiv.org/html/2504.17703v2) - July 2025

### Frameworks and Tools

- **TAMAS Benchmark**: Multi-agent adversarial testing framework
- **OpenMined PySyft**: Privacy-preserving ML
- **CrypTen**: PyTorch secure ML
- **Microsoft SEAL**: Homomorphic encryption
- **LangSmith**: Agent monitoring

### Additional Resources

- [Security and Privacy in Multi-Agent LLM Networks: Addressing Vulnerabilities](https://www.researchgate.net/publication/395308804_Security_and_Privacy_in_Multi-Agent_LLM_Networks_Addressing_Vulnerabilities) - ResearchGate 2025
- [The Emerged Security and Privacy of LLM Agent: A Survey](https://dl.acm.org/doi/10.1145/3773080) - ACM 2025
- [PriFLRC: A secure multi-party computation-based privacy-preserving federated learning](https://www.sciencedirect.com/science/article/abs/pii/S0925231225032461) - Neurocomputing 2026

---

**End of Document**
