# üöÄ Vercel Deployment Guide - AI Planning Assistant

## ‚úÖ What's Been Done

I've successfully converted the AI Planning Assistant to work on Vercel serverless functions. Here's what changed:

### **Architecture Changes:**

**Before (Broken on Vercel):**
```
Browser ‚Üí Node.js SDK (zhipuai) ‚Üí GLM API ‚ùå
         ‚Üí MCP Client (browser) ‚Üí Supabase ‚ùå
```

**After (Works on Vercel):**
```
Browser ‚Üí Vercel API Route ‚Üí GLM API ‚úÖ
         (Server-side)      ‚Üí Supabase ‚úÖ
```

### **Files Created/Modified:**

1. ‚úÖ **`api/ai-planning.ts`** - Vercel serverless function
2. ‚úÖ **`vercel.json`** - Updated with API route configuration
3. ‚úÖ **`PlanningAssistant.tsx`** - Now calls API route instead of service
4. ‚úÖ **`.env.vercel.example`** - Environment variables template
5. ‚úÖ **`AI_PLANNING_ASSISTANT_SETUP.md`** - Technical documentation

---

## üìã Deployment Checklist

### **Step 1: Add Environment Variables to Vercel**

Go to your Vercel Project Dashboard ‚Üí Settings ‚Üí Environment Variables and add:

```bash
# Required
GLM_API_KEY=b212e1592f9d401582181fc4bedfd34d.3UUZYyTqBF5tPDqK
SUPABASE_URL=https://avdgyrepwrvsvvwxrcc.supabase.co
SUPABASE_SERVICE_ROLE_KEY=eyJhbGci... (from your .env file)

# Optional (already configured)
VITE_CLERK_PUBLISHABLE_KEY=pk_test_...
CLERK_SECRET_KEY=sk_test_...
GROQ_API_KEY=gsk_...
VITE_GROQ_API_KEY=gsk_...
OPENAI_API_KEY=sk-proj-...
VITE_OPENAI_API_KEY=sk-proj-...
VITE_DEEPGRAM_API_KEY=bf6fe3ed...
```

**Important:** Select the appropriate environments:
- ‚úÖ Production
- ‚úÖ Preview (for PR deployments)
- ‚úÖ Development (for local testing)

### **Step 2: Deploy to Vercel**

```bash
# Option A: Using Vercel CLI
vercel --prod

# Option B: Using Git (automatic)
git add .
git commit -m "feat: add AI Planning Assistant with Vercel serverless API"
git push origin main
```

### **Step 3: Verify Deployment**

1. **Check the API endpoint:**
   ```
   https://your-domain.vercel.app/api/ai-planning
   ```

2. **Test with a curl request:**
   ```bash
   curl -X POST https://your-domain.vercel.app/api/ai-planning \
     -H "Content-Type: application/json" \
     -d '{
       "message": "What tasks do I have?",
       "context": {
         "date": "2025-01-16",
         "existingTimeblocks": []
       }
     }'
   ```

3. **Test in the app:**
   - Navigate to `/lifelock?section=timebox&subtab=morning`
   - Click the ‚ú® sparkle button
   - Send a message like "What tasks do I have?"

---

## üîß Troubleshooting

### **Error: "GLM_API_KEY not set"**

**Solution:** Add `GLM_API_KEY` to Vercel environment variables

### **Error: "Supabase credentials not configured"**

**Solution:** Add both `SUPABASE_URL` and `SUPABASE_SERVICE_ROLE_KEY` to Vercel

### **Error: "404 Not Found" on API route**

**Solution:** Make sure `vercel.json` has the correct rewrites:
```json
{
  "rewrites": [
    {
      "source": "/api/:path*",
      "destination": "/api/:path*"
    }
  ]
}
```

### **Error: "Module not found"**

**Solution:** Run `npm install` locally and redeploy

---

## üí° How It Works

### **Request Flow:**

```
1. User clicks ‚ú® button
   ‚Üì
2. Opens PlanningAssistant chat
   ‚Üì
3. User types: "I'm free from 9am to 5pm"
   ‚Üì
4. Client sends POST to /api/ai-planning
   {
     message: "I'm free from 9am to 5pm",
     context: {
       date: "2025-01-16",
       userId: "user_123",
       existingTimeblocks: [...],
       conversationHistory: [...]
     }
   }
   ‚Üì
5. Vercel serverless function (api/ai-planning.ts):
   - Fetches user's tasks from Supabase
   - Builds system prompt with context
   - Calls GLM API with user message
   - Parses AI response for TIMEBLOCK tags
   - Creates timeblocks in Supabase (if any)
   - Returns response to client
   ‚Üì
6. Client displays AI response
   ‚Üì
7. Timeblocks appear on timeline (if created)
```

### **Security:**

‚úÖ **API Keys** - Stored server-side, never exposed to browser
‚úÖ **Authentication** - Uses Clerk JWT for user verification
‚úÖ **Rate Limiting** - Can be added with Vercel Edge Config
‚úÖ **Input Validation** - All inputs validated server-side

---

## üìä Cost Monitoring

### **GLM API Usage:**
- Cost: ~¬•0.50 per 1M tokens
- Estimate: ~¬•5/month for moderate usage

**Monitoring:**
1. Go to https://open.bigmodel.cn/
2. Check your API usage dashboard
3. Set up alerts for high usage

### **Vercel Usage:**
- Free tier: 100GB bandwidth, 6,000 minutes
- Check Vercel dashboard ‚Üí Usage

### **Supabase Usage:**
- Free tier: 500MB storage, 2GB bandwidth
- Check Supabase dashboard ‚Üí Usage

---

## üîÑ Future Improvements

### **Option 1: Add Streaming (Real-time responses)**

Use Vercel AI SDK for streaming:
```typescript
import { streamText } from 'ai';

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: glm('glm-4-plus'),
    messages,
  });

  return result.toDataStreamResponse();
}
```

### **Option 2: Add Rate Limiting**

Use Vercel Edge Config:
```typescript
import { Ratelimit } from "@upstash/ratelimit";
import { Redis } from "@upstash/redis";

const ratelimit = new Ratelimit({
  redis: Redis.fromEnv(),
  limiter: Ratelimit.slidingWindow(10, "1 m"),
});

const { success } = await ratelimit.limit(userId);
if (!success) {
  return new Response("Too many requests", { status: 429 });
}
```

### **Option 3: Add Caching**

Cache common responses:
```typescript
import { Redis } from "@upstash/redis";

const cache = Redis.fromEnv();
const cached = await cache.get(cacheKey);
if (cached) return new Response(cached);
```

---

## üìù Summary

‚úÖ **Implemented:**
- Vercel serverless API function
- Secure API key handling
- Supabase integration
- Timeblock creation
- Voice input support
- Error handling

‚úÖ **Ready for:**
- Vercel deployment
- Production use
- Scaling to users

‚è≠Ô∏è **Next Steps:**
1. Add environment variables to Vercel
2. Deploy to Vercel
3. Test the planning assistant
4. Monitor API usage

---

**Need help?** Check the logs in Vercel Dashboard ‚Üí Your Project ‚Üí Logs
