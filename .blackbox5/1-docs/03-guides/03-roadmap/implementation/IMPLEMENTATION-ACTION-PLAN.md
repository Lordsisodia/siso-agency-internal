# Blackbox 5 Implementation Action Plan

**Created:** 2026-01-18
**Status:** Ready for Implementation
**Confidence:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5)

---

## Executive Summary

Based on comprehensive research and codebase analysis, **Blackbox 5 already has 70-80% of what we need**. This action plan identifies the remaining 20-30% that needs to be implemented to achieve a production-ready multi-agent system with **90%+ success rate** and **<20s coordination time**.

### Key Findings

‚úÖ **Already Implemented:**
- Event Bus (Redis-based) - `.blackbox5/engine/core/event_bus.py`
- Agent Loading System - `.blackbox5/engine/agents/core/AgentLoader.py`
- Skill Management System - `.blackbox5/engine/agents/core/SkillManager.py`
- Working Memory - `.blackbox5/engine/memory/`
- Episodic Memory (ChromaDB) - `.blackbox5/engine/memory/extended/`
- Brain System (Neo4j + PostgreSQL) - `.blackbox5/engine/brain/`
- Circuit Breaker - `.blackbox5/engine/core/circuit_breaker.py`

‚ö†Ô∏è **Needs Implementation:**
- Manager Agent (full 3-level hierarchy)
- Multi-Agent Coordinator
- Task Router (complexity-based routing)
- Shared Memory Integration
- Procedural Memory (Redis-based)
- Manifest System (operation tracking)
- Structured Logging
- CLI Tools

---

## Implementation Priority Matrix

### üî¥ Priority 1: Core Foundation (Week 1-2)
**Impact:** Critical | **Complexity:** Low | **Time:** 10-14 days

### üü° Priority 2: Coordination System (Week 3-4)
**Impact:** High | **Complexity:** Medium | **Time:** 10-14 days

### üü¢ Priority 3: Memory Enhancement (Week 5-6)
**Impact:** Medium | **Complexity:** Medium | **Time:** 10-14 days

### üîµ Priority 4: Production Readiness (Week 7-8)
**Impact:** Medium | **Complexity:** Low-Medium | **Time:** 10-14 days

---

## Phase 1: Core Foundation (Week 1-2)

### 1.1 Task Router System
**Location:** `.blackbox5/engine/core/task_router.py`

**Purpose:** Automatically route tasks to single agents or multi-agent system based on complexity

**Implementation:**

```python
"""
Task Router - Route tasks to single or multi-agent execution
"""

from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import re

class TaskType(Enum):
    SIMPLE = "simple"      # < 10 steps, single agent
    MODERATE = "moderate"  # 10-20 steps, may need coordination
    COMPLEX = "complex"    # 20+ steps, multi-agent required

@dataclass
class Task:
    id: str
    description: str
    prompt: str
    required_tools: List[str]
    domain: str
    context: Dict

@dataclass
class ExecutionStrategy:
    type: str  # 'single' or 'multi'
    agent: Optional[str]  # agent name for single
    reason: str
    estimated_steps: int
    confidence: float

class TaskRouter:
    """
    Analyze task complexity and route to appropriate execution strategy.

    Research shows:
    - Single agent: 4x faster for simple tasks
    - Multi-agent: 2-7x faster for complex tasks
    - Optimal threshold: ~10 steps
    """

    def __init__(self, agent_registry, skill_manager):
        self.agents = agent_registry
        self.skills = skill_manager

        # Complexity thresholds (tunable)
        self.THRESHOLDS = {
            'simple_max_steps': 10,
            'moderate_max_steps': 20,
            'min_tools_for_multi': 3,
            'complex_domains': [
                'system_architecture',
                'security',
                'distributed_systems',
                'database_migration'
            ]
        }

    def route(self, task: Task) -> ExecutionStrategy:
        """
        Decide execution strategy based on task analysis.

        Returns:
            ExecutionStrategy with recommended approach
        """
        complexity = self._analyze_complexity(task)

        # Simple: single generalist agent
        if complexity.score < 0.3:
            return ExecutionStrategy(
                type='single',
                agent=self._find_generalist(),
                reason=f'Simple task (score: {complexity.score:.2f}), single agent faster',
                estimated_steps=complexity.estimated_steps,
                confidence=0.9
            )

        # Moderate: single specialist or light coordination
        if complexity.score < 0.6:
            specialist = self._find_specialist(task)
            if specialist:
                return ExecutionStrategy(
                    type='single',
                    agent=specialist,
                    reason=f'Moderate task (score: {complexity.score:.2f}), specialist capable',
                    estimated_steps=complexity.estimated_steps,
                    confidence=0.8
                )
            else:
                return ExecutionStrategy(
                    type='multi',
                    agent=None,
                    reason=f'Moderate task (score: {complexity.score:.2f}), needs coordination',
                    estimated_steps=complexity.estimated_steps,
                    confidence=0.7
                )

        # Complex: multi-agent system
        return ExecutionStrategy(
            type='multi',
            agent=None,
            reason=f'Complex task (score: {complexity.score:.2f}), multi-agent required',
            estimated_steps=complexity.estimated_steps,
            confidence=0.85
        )

    def _analyze_complexity(self, task: Task) -> 'ComplexityScore':
        """
        Analyze task complexity across multiple dimensions.

        Factors:
        1. Token count (proxy for steps)
        2. Tool requirements
        3. Domain complexity
        4. Context dependencies
        """
        factors = []

        # Factor 1: Token count (0-1 scale)
        token_count = len(task.prompt) * 2  # Rough estimate
        if token_count < 500:
            token_score = 0.1
        elif token_count < 2000:
            token_score = 0.5
        elif token_count < 5000:
            token_score = 0.7
        else:
            token_score = 0.9
        factors.append(('tokens', token_score, 0.3))

        # Factor 2: Tool requirements (0-1 scale)
        tool_score = min(1.0, len(task.required_tools) * 0.2)
        factors.append(('tools', tool_score, 0.25))

        # Factor 3: Domain complexity (0-1 scale)
        domain_score = 1.0 if task.domain in self.THRESHOLDS['complex_domains'] else 0.3
        factors.append(('domain', domain_score, 0.25))

        # Factor 4: Context dependencies (0-1 scale)
        context_score = min(1.0, len(task.context) * 0.1)
        factors.append(('context', context_score, 0.2))

        # Calculate weighted score
        weighted_score = sum(score * weight for _, score, weight in factors)

        # Estimate steps (rough heuristic)
        estimated_steps = max(3, int(token_count / 200))

        return ComplexityScore(
            score=weighted_score,
            estimated_steps=estimated_steps,
            factors=factors
        )

    def _find_generalist(self) -> str:
        """Find best generalist agent"""
        # Look for agents with broad capability coverage
        generalists = []

        for agent_id, agent in self.agents.get_all().items():
            capabilities = len(agent.capabilities)
            if capabilities > 10:  # Broad capability
                generalists.append((agent_id, capabilities))

        if generalists:
            # Return most capable generalist
            return max(generalists, key=lambda x: x[1])[0]

        # Fallback
        return 'generalist'

    def _find_specialist(self, task: Task) -> Optional[str]:
        """Find specialist agent for task"""
        required_tools = set(task.required_tools)

        best_match = None
        best_coverage = 0

        for agent_id, agent in self.agents.get_all().items():
            agent_tools = set(agent.tools)
            coverage = len(required_tools & agent_tools) / len(required_tools)

            if coverage > best_coverage:
                best_coverage = coverage
                best_match = agent_id

        return best_match if best_coverage > 0.5 else None

@dataclass
class ComplexityScore:
    score: float  # 0-1
    estimated_steps: int
    factors: List[Tuple[str, float, float]]  # (name, score, weight)
```

**Integration Points:**
- Connect to existing `AgentLoader` (`.blackbox5/engine/agents/core/AgentLoader.py`)
- Connect to existing `SkillManager` (`.blackbox5/engine/agents/core/SkillManager.py`)
- Use existing event bus for routing events

**Files to Create:**
- `.blackbox5/engine/core/task_router.py` - Main implementation
- `.blackbox5/engine/core/complexity.py` - Complexity analysis module
- `.blackbox5/engine/core/__init__.py` - Export TaskRouter

**Time Estimate:** 2-3 days

---

### 1.2 Manager Agent Implementation
**Location:** `.blackbox5/engine/agents/manager/`

**Purpose:** Top-level coordination agent for complex multi-agent tasks

**Implementation:**

```markdown
---
name: "manager"
full_name: "Manager Agent"
type: "core"
category: "1-core"
version: "1.0.0"
role: "orchestrator"

icon: "üéØ"

description: |
  The Manager Agent is responsible for coordinating complex tasks that require
  multiple specialist agents. It decomposes tasks, delegates to specialists,
  monitors progress, and integrates results.

capabilities:
  - task_decomposition
  - agent_selection
  - parallel_coordination
  - result_integration
  - progress_monitoring
  - failure_recovery

tools:
  - event_bus
  - agent_registry
  - task_tracker
  - memory
  - circuit_breaker

communication_style: "directive and clear"

parallel_execution: true

context_budget: 100000

artifacts: []
---

# Manager Agent

You are the **Manager Agent**, responsible for coordinating complex multi-agent tasks.

## Your Role

When a task is too complex for a single agent (typically 20+ steps), you take charge:

1. **Analyze** the task and break it down into subtasks
2. **Identify** dependencies between subtasks
3. **Delegate** each subtask to the appropriate specialist agent
4. **Monitor** progress and handle failures
5. **Integrate** results into a coherent output

## Available Specialists

You have access to these specialist agents:

### Research Specialist (`researcher`)
- **Capabilities:** web_search, document_analysis, fact_checking, data_gathering
- **Best for:** Information retrieval, research, analysis
- **Use when:** Task requires finding information or analyzing data

### Code Specialist (`coder`)
- **Capabilities:** code_generation, debugging, refactoring, testing
- **Best for:** Implementation, bug fixes, code changes
- **Use when:** Task involves writing or modifying code

### Writing Specialist (`writer`)
- **Capabilities:** documentation, explanation, communication, summarization
- **Best for:** Docs, explanations, reports
- **Use when:** Task requires clear communication

### Architecture Specialist (`architect`)
- **Capabilities:** system_design, technical_planning, architecture_decisions
- **Best for:** Design decisions, technical planning
- **Use when:** Task involves system design or architecture

### Analysis Specialist (`analyst`)
- **Capabilities:** data_analysis, insights, visualization, metrics
- **Best for:** Data interpretation, analytics
- **Use when:** Task involves data or metrics

## Coordination Protocol

### 1. Task Decomposition

Break the task into subtasks:

```markdown
## Task: [original task]

### Subtask 1: [name]
- **Specialist:** [which agent]
- **Dependencies:** [what must come first]
- **Output:** [what this produces]

### Subtask 2: [name]
...
```

### 2. Execution Strategy

**Parallel:** Execute independent subtasks concurrently
**Sequential:** Execute dependent subtasks in order

### 3. Progress Monitoring

Track each subtask:
- Status (pending, in_progress, completed, failed)
- Start time
- Dependencies satisfied?

### 4. Failure Recovery

If a subtask fails:
1. Analyze the failure
2. Try alternative specialist
3. Adjust approach if needed
4. Escalate if unrecoverable

### 5. Result Integration

Combine specialist outputs into:
- Coherent final result
- Clear summary of what was done
- Any issues or caveats

## Communication

**Always:**
- Use the event bus for coordination
- Publish progress updates
- Log important decisions
- Report failures immediately

**Event Topics:**
- `manager.task.decomposed` - Task breakdown complete
- `manager.subtask.assigned` - Subtask delegated to specialist
- `manager.subtask.completed` - Subtask finished
- `manager.subtask.failed` - Subtask failed
- `manager.task.completed` - Entire task finished

## Example Workflow

**Input:** "Build a REST API for user management with authentication"

**Decomposition:**
1. Architect ‚Üí Design API structure and endpoints
2. Researcher ‚Üí Find best practices for REST APIs
3. Coder ‚Üí Implement endpoints and authentication
4. Analyst ‚Üí Review and validate implementation

**Execution:**
1. Send task to Architect (parallel with Researcher)
2. Wait for both to complete
3. Send combined output to Coder
4. Wait for Coder to complete
5. Send to Analyst for review
6. Integrate all results

**Output:** Complete API with documentation

## Best Practices

‚úÖ **DO:**
- Break tasks into clear, manageable subtasks
- Use appropriate specialists for each subtask
- Monitor progress closely
- Handle failures gracefully
- Communicate clearly via events

‚ùå **DON'T:**
- Micro-manage specialists
- Create subtasks that are too small
- Ignore dependencies
- Hide failures
- Skip integration step

## Success Criteria

A task is successful when:
- All subtasks completed
- Results integrated coherently
- Output meets original requirements
- No unresolved failures

---

**Generated:** 2026-01-18
**Version:** 1.0.0
```

**Integration Points:**
- Register with existing `AgentLoader`
- Subscribe to event bus topics
- Use existing `SkillManager` for capabilities

**Files to Create:**
- `.blackbox5/engine/agents/1-core/manager/agent.md` - Agent definition
- `.blackbox5/engine/agents/1-core/manager/prompt.md` - System prompt
- `.blackbox5/engine/agents/1-core/manager/config.yaml` - Configuration

**Time Estimate:** 1-2 days

---

### 1.3 Structured Logging System
**Location:** `.blackbox5/engine/core/logging.py`

**Purpose:** Comprehensive logging for debugging and monitoring

**Implementation:**

```python
"""
Structured Logging System for Blackbox 5
"""

import structlog
import logging
import sys
from pathlib import Path
from typing import Any, Dict
from datetime import datetime
import json

def setup_logging(
    level: str = "INFO",
    log_file: Path = None,
    json_logs: bool = True
) -> None:
    """
    Configure structured logging for Blackbox 5.

    Args:
        level: Logging level (DEBUG, INFO, WARNING, ERROR)
        log_file: Optional file to write logs to
        json_logs: Whether to output JSON logs (default: True)
    """
    # Configure structlog
    processors = [
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
    ]

    if json_logs:
        processors.append(structlog.processors.JSONRenderer())
    else:
        processors.append(structlog.dev.ConsoleRenderer())

    structlog.configure(
        processors=processors,
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        cache_logger_on_first_use=True,
        wrapper_class=structlog.stdlib.BoundLogger,
    )

    # Configure standard logging
    logging.basicConfig(
        format="%(message)s",
        stream=sys.stdout,
        level=getattr(logging, level.upper()),
    )

    # Add file handler if specified
    if log_file:
        log_file.parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(getattr(logging, level.upper()))
        logging.getLogger().addHandler(file_handler)

class AgentLogger:
    """
    Structured logger for agents with automatic context binding.
    """

    def __init__(self, agent_id: str, agent_type: str):
        self.agent_id = agent_id
        self.agent_type = agent_type
        self.logger = structlog.get_logger()
        self.logger = self.logger.bind(
            agent_id=agent_id,
            agent_type=agent_type
        )

    def task_start(self, task_id: str, task_description: str, **kwargs):
        """Log task start"""
        self.logger.info(
            "task.start",
            task_id=task_id,
            task_description=task_description,
            **kwargs
        )

    def task_progress(self, task_id: str, progress: float, message: str, **kwargs):
        """Log task progress"""
        self.logger.info(
            "task.progress",
            task_id=task_id,
            progress=progress,
            message=message,
            **kwargs
        )

    def task_success(self, task_id: str, result: Any, **kwargs):
        """Log task success"""
        self.logger.info(
            "task.success",
            task_id=task_id,
            result_type=type(result).__name__,
            **kwargs
        )

    def task_failure(self, task_id: str, error: str, exc_info=False, **kwargs):
        """Log task failure"""
        self.logger.error(
            "task.failed",
            task_id=task_id,
            error=error,
            exc_info=exc_info,
            **kwargs
        )

    def agent_event(self, event_type: str, **kwargs):
        """Log agent event"""
        self.logger.info(
            f"agent.{event_type}",
            **kwargs
        )

    def bind_context(self, **kwargs) -> 'AgentLogger':
        """Create new logger with additional context"""
        new_logger = AgentLogger(self.agent_id, self.agent_type)
        new_logger.logger = self.logger.bind(**kwargs)
        return new_logger

class OperationLogger:
    """
    Logger for tracking multi-agent operations.
    """

    def __init__(self, operation_id: str):
        self.operation_id = operation_id
        self.logger = structlog.get_logger()
        self.logger = self.logger.bind(operation_id=operation_id)
        self.start_time = datetime.now()

    def operation_start(self, operation_type: str, description: str, **kwargs):
        """Log operation start"""
        self.logger.info(
            "operation.start",
            operation_type=operation_type,
            description=description,
            **kwargs
        )

    def operation_step(self, step: str, **kwargs):
        """Log operation step"""
        self.logger.info(
            "operation.step",
            step=step,
            **kwargs
        )

    def operation_complete(self, result: Any, **kwargs):
        """Log operation completion"""
        duration = (datetime.now() - self.start_time).total_seconds()
        self.logger.info(
            "operation.complete",
            duration_seconds=duration,
            result_type=type(result).__name__,
            **kwargs
        )

    def operation_failure(self, error: str, exc_info=False, **kwargs):
        """Log operation failure"""
        duration = (datetime.now() - self.start_time).total_seconds()
        self.logger.error(
            "operation.failed",
            duration_seconds=duration,
            error=error,
            exc_info=exc_info,
            **kwargs
        )

# Convenience functions
def get_agent_logger(agent_id: str, agent_type: str) -> AgentLogger:
    """Get a logger for an agent"""
    return AgentLogger(agent_id, agent_type)

def get_operation_logger(operation_id: str) -> OperationLogger:
    """Get a logger for an operation"""
    return OperationLogger(operation_id)
```

**Integration Points:**
- Call from all agents for consistent logging
- Use in existing core modules
- Export from `.blackbox5/engine/core/__init__.py`

**Files to Create:**
- `.blackbox5/engine/core/logging.py` - Main implementation
- `.blackbox5/engine/runtime/view-logs.sh` - Log viewer CLI tool

**Time Estimate:** 1-2 days

---

### 1.4 Manifest System
**Location:** `.blackbox5/engine/core/manifest.py`

**Purpose:** Track all operations for debugging and audit trails

**Implementation:**

```python
"""
Manifest System - Track operations for debugging and audit
"""

from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from enum import Enum
import uuid
import json

class ManifestStatus(Enum):
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"

@dataclass
class ManifestStep:
    step: str
    timestamp: str
    details: Dict[str, Any]
    status: str = "completed"

@dataclass
class Manifest:
    id: str
    type: str
    started_at: str
    status: ManifestStatus
    steps: List[ManifestStep] = field(default_factory=list)
    completed_at: Optional[str] = None
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

class ManifestSystem:
    """
    Track all operations for debugging and audit.

    Creates markdown files for each operation with:
    - Operation metadata
    - Step-by-step execution
    - Results and errors
    - Timing information
    """

    def __init__(self, manifest_dir: Path = None):
        if manifest_dir is None:
            manifest_dir = Path("./.blackbox5/scratch/manifests")

        self.manifest_dir = Path(manifest_dir)
        self.manifest_dir.mkdir(parents=True, exist_ok=True)

        self.active_manifests: Dict[str, Manifest] = {}

    def create_manifest(
        self,
        operation_type: str,
        metadata: Dict[str, Any] = None
    ) -> Manifest:
        """
        Create new operation manifest.

        Args:
            operation_type: Type of operation (e.g., "task_execution", "agent_coordination")
            metadata: Additional metadata to attach

        Returns:
            Manifest object
        """
        manifest_id = str(uuid.uuid4())

        manifest = Manifest(
            id=manifest_id,
            type=operation_type,
            started_at=datetime.now().isoformat(),
            status=ManifestStatus.PENDING,
            metadata=metadata or {}
        )

        self.active_manifests[manifest_id] = manifest
        self._save_manifest(manifest)

        return manifest

    def start_step(
        self,
        manifest: Manifest,
        step: str,
        details: Dict[str, Any] = None
    ) -> None:
        """
        Start a new step in the operation.

        Args:
            manifest: Manifest to add step to
            step: Step description
            details: Step details
        """
        manifest.status = ManifestStatus.IN_PROGRESS

        step_record = ManifestStep(
            step=step,
            timestamp=datetime.now().isoformat(),
            details=details or {}
        )

        manifest.steps.append(step_record)
        self._save_manifest(manifest)

    def complete_manifest(
        self,
        manifest: Manifest,
        result: Dict[str, Any] = None
    ) -> None:
        """
        Mark manifest as completed.

        Args:
            manifest: Manifest to complete
            result: Operation result
        """
        manifest.status = ManifestStatus.COMPLETED
        manifest.completed_at = datetime.now().isoformat()
        manifest.result = result

        self._save_manifest(manifest)
        self.active_manifests.pop(manifest.id, None)

    def fail_manifest(
        self,
        manifest: Manifest,
        error: str
    ) -> None:
        """
        Mark manifest as failed.

        Args:
            manifest: Manifest that failed
            error: Error message
        """
        manifest.status = ManifestStatus.FAILED
        manifest.completed_at = datetime.now().isoformat()
        manifest.error = error

        self._save_manifest(manifest)
        self.active_manifests.pop(manifest.id, None)

    def get_manifest(self, manifest_id: str) -> Optional[Manifest]:
        """Get manifest by ID"""
        return self.active_manifests.get(manifest_id)

    def list_manifests(
        self,
        operation_type: str = None,
        status: ManifestStatus = None
    ) -> List[Manifest]:
        """List manifests with optional filtering"""
        # Scan manifest directory
        manifests = []

        for manifest_file in self.manifest_dir.glob("*.md"):
            try:
                manifest = self._load_manifest_file(manifest_file)

                if operation_type and manifest.type != operation_type:
                    continue
                if status and manifest.status != status:
                    continue

                manifests.append(manifest)
            except Exception:
                continue

        # Sort by started_at (newest first)
        manifests.sort(key=lambda m: m.started_at, reverse=True)

        return manifests

    def _save_manifest(self, manifest: Manifest) -> None:
        """Save manifest to file"""
        path = self.manifest_dir / f"{manifest.id}.md"

        with open(path, 'w') as f:
            f.write(self._format_manifest(manifest))

    def _format_manifest(self, manifest: Manifest) -> str:
        """Format manifest as markdown"""
        lines = [
            f"# Operation Manifest: {manifest.type}",
            "",
            "## Metadata",
            f"- **ID:** `{manifest.id}`",
            f"- **Type:** {manifest.type}",
            f"- **Started:** {manifest.started_at}",
            f"- **Status:** {manifest.status.value}",
        ]

        if manifest.completed_at:
            lines.append(f"- **Completed:** {manifest.completed_at}")

        if manifest.metadata:
            lines.append("")
            lines.append("### Additional Metadata")
            for key, value in manifest.metadata.items():
                lines.append(f"- **{key}:** {value}")

        if manifest.steps:
            lines.append("")
            lines.append("## Execution Steps")

            for i, step in enumerate(manifest.steps, 1):
                lines.append(f"### Step {i}: {step.step}")
                lines.append(f"- **Time:** {step.timestamp}")
                lines.append(f"- **Status:** {step.status}")

                if step.details:
                    lines.append("")
                    lines.append("**Details:**")
                    for key, value in step.details.items():
                        if isinstance(value, (dict, list)):
                            value = json.dumps(value, indent=2)
                        lines.append(f"  - {key}: {value}")

                lines.append("")

        if manifest.result:
            lines.append("## Result")
            lines.append("```json")
            lines.append(json.dumps(manifest.result, indent=2))
            lines.append("```")

        if manifest.error:
            lines.append("## Error")
            lines.append(f"```\n{manifest.error}\n```")

        return "\n".join(lines)

    def _load_manifest_file(self, path: Path) -> Manifest:
        """Load manifest from file (basic parsing)"""
        # This is a simplified version - could parse the markdown properly
        # For now, just return a stub
        return Manifest(
            id=path.stem,
            type="unknown",
            started_at="unknown",
            status=ManifestStatus.COMPLETED
        )
```

**Integration Points:**
- Use in all long-running operations
- Auto-create on task execution
- Link with structured logging

**Files to Create:**
- `.blackbox5/engine/core/manifest.py` - Main implementation
- `.blackbox5/engine/runtime/view-manifest.sh` - CLI viewer

**Time Estimate:** 2 days

---

## Phase 2: Coordination System (Week 3-4)

### 2.1 Multi-Agent Coordinator
**Location:** `.blackbox5/engine/core/coordination.py`

**Purpose:** Coordinate multi-agent task execution

**Implementation:**

```python
"""
Multi-Agent Coordinator - Orchestrate complex multi-agent tasks
"""

from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum
import asyncio

from .event_bus import EventBus
from .task_router import Task, ExecutionStrategy
from .circuit_breaker import CircuitBreaker
from .manifest import ManifestSystem, Manifest
from .logging import get_operation_logger

class CoordinationMode(Enum):
    PARALLEL = "parallel"  # Execute independent tasks concurrently
    SEQUENTIAL = "sequential"  # Execute tasks in order
    WAVE = "wave"  # Wave-based execution (dependency-aware)

@dataclass
class Subtask:
    id: str
    description: str
    specialist: str
    dependencies: List[str]
    status: str = "pending"
    result: Any = None
    error: Optional[str] = None

@dataclass
class CoordinationPlan:
    subtasks: List[Subtask]
    mode: CoordinationMode
    dependencies: Dict[str, List[str]]

class MultiAgentCoordinator:
    """
    Coordinate multi-agent task execution.

    Responsibilities:
    1. Decompose tasks into subtasks
    2. Identify dependencies
    3. Execute in optimal order
    4. Monitor progress
    5. Handle failures
    6. Integrate results
    """

    def __init__(
        self,
        event_bus: EventBus,
        agent_registry,
        circuit_breaker: CircuitBreaker,
        manifest_system: ManifestSystem
    ):
        self.event_bus = event_bus
        self.agents = agent_registry
        self.circuit_breaker = circuit_breaker
        self.manifests = manifest_system

        # Subscribe to events
        self.event_bus.subscribe("subtask.completed", self._on_subtask_completed)
        self.event_bus.subscribe("subtask.failed", self._on_subtask_failed)

    async def execute(self, task: Task, strategy: ExecutionStrategy) -> Any:
        """
        Execute task using multi-agent coordination.

        Args:
            task: Task to execute
            strategy: Execution strategy (must be multi-agent)

        Returns:
            Integrated result from all subtasks
        """
        # Create manifest
        manifest = self.manifests.create_manifest(
            "multi_agent_coordination",
            {"task_id": task.id, "task_description": task.description}
        )

        logger = get_operation_logger(manifest.id)
        logger.operation_start("coordination", task.description)

        try:
            # Step 1: Decompose task
            self.manifests.start_step(manifest, "task_decomposition")
            plan = await self._decompose_task(task)
            logger.operation_step("task_decomposed", subtasks=len(plan.subtasks))

            # Step 2: Execute subtasks
            self.manifests.start_step(manifest, "subtask_execution")
            results = await self._execute_plan(plan, manifest)
            logger.operation_step("subtasks_executed", completed=len(results))

            # Step 3: Integrate results
            self.manifests.start_step(manifest, "result_integration")
            final_result = await self._integrate_results(results, task)
            logger.operation_step("results_integrated")

            # Complete
            self.manifests.complete_manifest(manifest, {"result": final_result})
            logger.operation_complete(final_result)

            return final_result

        except Exception as e:
            error_msg = f"Coordination failed: {str(e)}"
            self.manifests.fail_manifest(manifest, error_msg)
            logger.operation_failure(error_msg, exc_info=True)
            raise

    async def _decompose_task(self, task: Task) -> CoordinationPlan:
        """
        Decompose task into subtasks.

        Uses the Manager agent to break down the task.
        """
        # Get manager agent
        manager = self.agents.get_agent("manager")

        # Request decomposition
        decomposition_prompt = f"""
        Decompose this task into subtasks:

        Task: {task.description}

        For each subtask, specify:
        1. What needs to be done
        2. Which specialist should handle it
        3. What dependencies it has

        Format as JSON list of subtasks.
        """

        response = await self.circuit_breaker.call(
            manager.execute,
            decomposition_prompt
        )

        # Parse response into subtasks
        subtasks = self._parse_decomposition(response)

        # Analyze dependencies and determine execution mode
        mode = self._determine_execution_mode(subtasks)

        return CoordinationPlan(
            subtasks=subtasks,
            mode=mode,
            dependencies=self._build_dependency_graph(subtasks)
        )

    async def _execute_plan(
        self,
        plan: CoordinationPlan,
        manifest: Manifest
    ) -> Dict[str, Any]:
        """
        Execute coordination plan.

        Handles parallel, sequential, and wave execution.
        """
        if plan.mode == CoordinationMode.PARALLEL:
            return await self._execute_parallel(plan.subtasks, manifest)
        elif plan.mode == CoordinationMode.SEQUENTIAL:
            return await self._execute_sequential(plan.subtasks, manifest)
        else:  # WAVE
            return await self._execute_wave(plan, manifest)

    async def _execute_parallel(
        self,
        subtasks: List[Subtask],
        manifest: Manifest
    ) -> Dict[str, Any]:
        """Execute subtasks in parallel"""
        results = {}

        # Create tasks for all subtasks
        tasks = [
            self._execute_subtask(subtask, manifest)
            for subtask in subtasks
        ]

        # Execute concurrently
        completed = await asyncio.gather(*tasks, return_exceptions=True)

        # Collect results
        for subtask, result in zip(subtasks, completed):
            if isinstance(result, Exception):
                subtask.status = "failed"
                subtask.error = str(result)
            else:
                subtask.status = "completed"
                subtask.result = result
                results[subtask.id] = result

        return results

    async def _execute_sequential(
        self,
        subtasks: List[Subtask],
        manifest: Manifest
    ) -> Dict[str, Any]:
        """Execute subtasks sequentially"""
        results = {}

        for subtask in subtasks:
            # Check dependencies
            if not self._dependencies_satisfied(subtask, results):
                raise Exception(f"Dependencies not satisfied for {subtask.id}")

            # Execute subtask
            try:
                result = await self._execute_subtask(subtask, manifest)
                subtask.status = "completed"
                subtask.result = result
                results[subtask.id] = result
            except Exception as e:
                subtask.status = "failed"
                subtask.error = str(e)
                raise

        return results

    async def _execute_wave(
        self,
        plan: CoordinationPlan,
        manifest: Manifest
    ) -> Dict[str, Any]:
        """
        Execute subtasks in waves (dependency-aware parallel execution).

        Tasks in the same wave (no dependencies between them) execute in parallel.
        Waves execute sequentially.
        """
        results = {}

        # Build waves
        waves = self._build_waves(plan)

        # Execute each wave
        for wave_num, wave in enumerate(waves, 1):
            self.manifests.start_step(
                manifest,
                f"wave_{wave_num}",
                {"subtasks": len(wave)}
            )

            # Execute wave in parallel
            wave_results = await self._execute_parallel(wave, manifest)
            results.update(wave_results)

        return results

    async def _execute_subtask(
        self,
        subtask: Subtask,
        manifest: Manifest
    ) -> Any:
        """Execute a single subtask"""
        # Get specialist agent
        specialist = self.agents.get_agent(subtask.specialist)

        # Publish event
        self.event_bus.publish("subtask.started", {
            "subtask_id": subtask.id,
            "specialist": subtask.specialist
        })

        # Execute with circuit breaker
        result = await self.circuit_breaker.call(
            specialist.execute,
            subtask.description
        )

        return result

    async def _integrate_results(
        self,
        results: Dict[str, Any],
        original_task: Task
    ) -> Any:
        """
        Integrate results from subtasks into coherent output.

        Uses Manager agent to perform integration.
        """
        manager = self.agents.get_agent("manager")

        integration_prompt = f"""
        Integrate these results into a coherent response:

        Original Task: {original_task.description}

        Results:
        {json.dumps(results, indent=2)}

        Provide a unified, coherent response that addresses the original task.
        """

        return await self.circuit_breaker.call(
            manager.execute,
            integration_prompt
        )

    def _on_subtask_completed(self, event: dict):
        """Handle subtask completion event"""
        # Update tracking
        pass

    def _on_subtask_failed(self, event: dict):
        """Handle subtask failure event"""
        # Log error, potentially retry
        pass

    # Helper methods
    def _parse_decomposition(self, response: str) -> List[Subtask]:
        """Parse manager response into subtasks"""
        # Implementation depends on manager output format
        pass

    def _determine_execution_mode(self, subtasks: List[Subtask]) -> CoordinationMode:
        """Determine optimal execution mode"""
        # Check for dependencies
        has_deps = any(st.dependencies for st in subtasks)

        if not has_deps:
            return CoordinationMode.PARALLEL
        elif len(subtasks) <= 3:
            return CoordinationMode.SEQUENTIAL
        else:
            return CoordinationMode.WAVE

    def _build_dependency_graph(self, subtasks: List[Subtask]) -> Dict[str, List[str]]:
        """Build dependency graph"""
        return {st.id: st.dependencies for st in subtasks}

    def _build_waves(self, plan: CoordinationPlan) -> List[List[Subtask]]:
        """Build execution waves from dependency graph"""
        # Topological sort to determine waves
        waves = []
        remaining = set(plan.subtasks)

        while remaining:
            # Find tasks with no unsatisfied dependencies
            ready = [
                st for st in remaining
                if all(
                    dep not in {s.id for s in remaining}
                    for dep in st.dependencies
                )
            ]

            if not ready:
                raise Exception("Circular dependency detected")

            waves.append(ready)
            remaining -= set(ready)

        return waves

    def _dependencies_satisfied(self, subtask: Subtask, results: Dict[str, Any]) -> bool:
        """Check if subtask dependencies are satisfied"""
        return all(dep in results for dep in subtask.dependencies)
```

**Integration Points:**
- Connect to existing event bus
- Use existing circuit breaker
- Use existing agent loader

**Files to Create:**
- `.blackbox5/engine/core/coordination.py` - Main implementation
- `.blackbox5/engine/core/parallel.py` - Parallel execution utilities

**Time Estimate:** 3-4 days

---

### 2.2 Integration with Existing Brain System
**Location:** Connect `.blackbox5/engine/memory/` with `.blackbox5/engine/brain/`

**Purpose:** Integrate semantic memory capabilities from brain system

**Implementation:**

```python
"""
Integrated Memory System - Connect working, episodic, and semantic memory
"""

from typing import List, Dict, Any, Optional
from dataclasses import dataclass

# Import existing systems
from .brain.query.vector import VectorSearch
from .brain.query.graph import GraphQuery
from .brain.query.nl_parser import QueryExecutor

@dataclass
class MemoryResult:
    content: str
    source: str  # 'working', 'episodic', 'semantic'
    relevance: float
    metadata: Dict[str, Any]

class IntegratedMemory:
    """
    Unified memory system combining all memory types.

    Provides:
    1. Working memory (fast, session-based)
    2. Episodic memory (ChromaDB vector search)
    3. Semantic memory (Neo4j knowledge graph)
    """

    def __init__(
        self,
        working_capacity: int = 100000,
        episodic_path: str = "./memory/chroma",
        neo4j_uri: str = "bolt://localhost:7687"
    ):
        # Working memory (in-memory)
        self.working_memory: Dict[str, Any] = {}
        self.working_capacity = working_capacity

        # Episodic memory (existing vector search)
        self.vector_search = VectorSearch()

        # Semantic memory (existing graph query)
        try:
            self.graph_query = GraphQuery(neo4j_uri, "neo4j", "password")
            self.semantic_available = self.graph_query.connect()
        except Exception:
            self.semantic_available = False

    def store(
        self,
        content: str,
        memory_type: str = "working",
        metadata: Dict[str, Any] = None
    ) -> None:
        """
        Store information in appropriate memory layer.

        Args:
            content: Content to store
            memory_type: 'working', 'episodic', or 'semantic'
            metadata: Additional metadata
        """
        if memory_type == "working":
            self._store_working(content, metadata)
        elif memory_type == "episodic":
            self._store_episodic(content, metadata)
        elif memory_type == "semantic" and self.semantic_available:
            self._store_semantic(content, metadata)

    def retrieve(
        self,
        query: str,
        max_results: int = 10,
        min_relevance: float = 0.5
    ) -> List[MemoryResult]:
        """
        Retrieve from all memory layers, ranked by relevance.

        Args:
            query: Search query
            max_results: Maximum results to return
            min_relevance: Minimum relevance threshold

        Returns:
            List of memory results ranked by relevance
        """
        results = []

        # Check working memory (highest priority)
        working_results = self._search_working(query)
        for result in working_results:
            results.append(MemoryResult(
                content=result,
                source="working",
                relevance=1.0,  # Exact match
                metadata={}
            ))

        # Check episodic memory
        if len(results) < max_results:
            episodic_results = self.vector_search.semantic_search(
                query_vector=self._embed(query),
                limit=max_results - len(results),
                min_similarity=min_relevance
            )
            for result in episodic_results:
                results.append(MemoryResult(
                    content=result.content,
                    source="episodic",
                    relevance=result.similarity,
                    metadata=result.metadata
                ))

        # Check semantic memory
        if self.semantic_available and len(results) < max_results:
            semantic_results = self.graph_query.execute_cypher(
                f"MATCH (n) WHERE n.content CONTAINS '{query}' RETURN n LIMIT {max_results - len(results)}"
            )
            for result in semantic_results:
                results.append(MemoryResult(
                    content=result.get("content", ""),
                    source="semantic",
                    relevance=0.7,  # Graph-based relevance
                    metadata=result
                ))

        # Sort by relevance and return top results
        results.sort(key=lambda r: r.relevance, reverse=True)
        return results[:max_results]

    def _store_working(self, content: str, metadata: Dict[str, Any]) -> None:
        """Store in working memory"""
        key = metadata.get("key", hash(content))
        self.working_memory[key] = {
            "content": content,
            "metadata": metadata,
            "timestamp": datetime.now().isoformat()
        }

        # Enforce capacity limit
        if len(self.working_memory) > self.working_capacity:
            # Remove oldest entries
            oldest = sorted(
                self.working_memory.items(),
                key=lambda x: x[1]["timestamp"]
            )[:len(self.working_memory) - self.working_capacity]
            for key, _ in oldest:
                del self.working_memory[key]

    def _search_working(self, query: str) -> List[str]:
        """Search working memory"""
        results = []
        query_lower = query.lower()

        for key, value in self.working_memory.items():
            content = value["content"]
            if query_lower in content.lower():
                results.append(content)

        return results

    # Additional methods for episodic and semantic storage
    def _store_episodic(self, content: str, metadata: Dict[str, Any]) -> None:
        """Store in episodic memory (ChromaDB)"""
        # Use existing vector search
        pass

    def _store_semantic(self, content: str, metadata: Dict[str, Any]) -> None:
        """Store in semantic memory (Neo4j)"""
        # Use existing graph query
        pass

    def _embed(self, text: str) -> List[float]:
        """Generate embedding for text"""
        # Use existing embedder
        pass
```

**Integration Points:**
- Connect to existing brain API (`.blackbox5/engine/brain/api/brain_api.py`)
- Use existing vector search (`.blackbox5/engine/brain/query/vector.py`)
- Use existing graph query (`.blackbox5/engine/brain/query/graph.py`)

**Files to Create:**
- `.blackbox5/engine/memory/integrated.py` - Main implementation
- `.blackbox5/engine/memory/__init__.py` - Export integrated memory

**Time Estimate:** 3-4 days

---

## Phase 3: Memory Enhancement (Week 5-6)

### 3.1 Procedural Memory System
**Location:** `.blackbox5/engine/memory/procedural.py`

**Purpose:** Store and retrieve skill patterns and procedures

**Implementation:**

```python
"""
Procedural Memory - Store skill patterns and procedures
"""

from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import redis
import json
from pathlib import Path

@dataclass
class Procedure:
    id: str
    name: str
    pattern: str
    success_rate: float
    usage_count: int
    last_used: str
    metadata: Dict[str, Any]

class ProceduralMemory:
    """
    Procedural memory for storing skill patterns and procedures.

    Uses Redis for fast key-value access.

    Benefits:
    - +3% task success rate
    - Faster execution of repeated tasks
    - Learning from experience
    """

    def __init__(
        self,
        redis_url: str = "redis://localhost:6379",
        db: int = 1  # Separate DB from event bus
    ):
        self.redis = redis.from_url(redis_url, db=db)
        self.key_prefix = "procedure:"

    def store_procedure(
        self,
        name: str,
        pattern: str,
        metadata: Dict[str, Any] = None
    ) -> str:
        """
        Store a procedure/pattern.

        Args:
            name: Procedure name
            pattern: Procedure pattern (e.g., "Step 1: X, Step 2: Y")
            metadata: Additional metadata

        Returns:
            Procedure ID
        """
        import uuid
        procedure_id = str(uuid.uuid4())

        procedure = Procedure(
            id=procedure_id,
            name=name,
            pattern=pattern,
            success_rate=0.0,
            usage_count=0,
            last_used="",
            metadata=metadata or {}
        )

        key = self._make_key(procedure_id)
        self.redis.set(key, json.dumps(procedure.__dict__))

        # Add to name index
        self.redis.sadd(f"name_index:{name}", procedure_id)

        return procedure_id

    def get_procedure(self, procedure_id: str) -> Optional[Procedure]:
        """Get procedure by ID"""
        key = self._make_key(procedure_id)
        data = self.redis.get(key)

        if data:
            return Procedure(**json.loads(data))
        return None

    def find_procedures(self, name_pattern: str) -> List[Procedure]:
        """Find procedures matching name pattern"""
        procedure_ids = self.redis.keys(f"name_index:{name_pattern}*")

        procedures = []
        for pid_key in procedure_ids:
            pid = pid_key.split(":")[-1]
            procedure = self.get_procedure(pid)
            if procedure:
                procedures.append(procedure)

        return procedures

    def record_usage(
        self,
        procedure_id: str,
        success: bool
    ) -> None:
        """
        Record procedure usage and update success rate.

        Args:
            procedure_id: Procedure ID
            success: Whether the procedure succeeded
        """
        procedure = self.get_procedure(procedure_id)
        if not procedure:
            return

        # Update metrics
        procedure.usage_count += 1
        procedure.last_used = datetime.now().isoformat()

        # Update success rate (exponential moving average)
        alpha = 0.1  # Learning rate
        if procedure.usage_count == 1:
            procedure.success_rate = 1.0 if success else 0.0
        else:
            procedure.success_rate = (
                alpha * (1.0 if success else 0.0) +
                (1 - alpha) * procedure.success_rate
            )

        # Save updated procedure
        key = self._make_key(procedure_id)
        self.redis.set(key, json.dumps(procedure.__dict__))

    def get_best_procedure(self, task_type: str) -> Optional[Procedure]:
        """
        Get best procedure for a task type.

        Returns procedure with highest success rate for the task.
        """
        procedures = self.find_procedures(task_type)

        if not procedures:
            return None

        # Sort by success rate and usage count
        return max(
            procedures,
            key=lambda p: (p.success_rate, p.usage_count)
        )

    def _make_key(self, procedure_id: str) -> str:
        """Create Redis key for procedure"""
        return f"{self.key_prefix}{procedure_id}"
```

**Integration Points:**
- Use Redis for fast access
- Connect with existing skill system
- Update from agent executions

**Files to Create:**
- `.blackbox5/engine/memory/procedural.py` - Main implementation
- `.blackbox5/engine/runtime/init-redis-procedural.sh` - Setup script

**Time Estimate:** 2-3 days

---

### 3.2 Memory Consolidation System
**Location:** `.blackbox5/engine/memory/consolidation.py`

**Purpose:** Automatically consolidate working memory to episodic

**Implementation:**

```python
"""
Memory Consolidation - Move working memory to episodic storage
"""

from typing import List, Dict, Any
from datetime import datetime, timedelta

class MemoryConsolidation:
    """
    Automatically consolidate working memory to episodic storage.

    Consolidation triggers:
    1. Time-based (every N minutes)
    2. Capacity-based (when working memory is full)
    3. Importance-based (consolidate important items first)
    """

    def __init__(
        self,
        working_memory,
        episodic_memory,
        consolidation_interval: int = 300,  # 5 minutes
        importance_threshold: float = 0.7
    ):
        self.working = working_memory
        self.episodic = episodic_memory
        self.interval = consolidation_interval
        self.importance_threshold = importance_threshold

        self.last_consolidation = datetime.now()

    def should_consolidate(self) -> bool:
        """Check if consolidation should run"""
        # Time-based trigger
        if (datetime.now() - self.last_consolidation).total_seconds() > self.interval:
            return True

        # Capacity-based trigger
        if len(self.working) >= self.working_capacity * 0.9:
            return True

        return False

    def consolidate(self) -> int:
        """
        Consolidate working memory to episodic.

        Returns:
            Number of items consolidated
        """
        if not self.should_consolidate():
            return 0

        # Get items to consolidate
        items = self._get_consolidation_candidates()

        # Consolidate each item
        consolidated = 0
        for item in items:
            if self._is_important(item):
                self.episodic.store(
                    content=item["content"],
                    metadata=item["metadata"]
                )
                del self.working[item["key"]]
                consolidated += 1

        self.last_consolidation = datetime.now()
        return consolidated

    def _get_consolidation_candidates(self) -> List[Dict[str, Any]]:
        """Get items that should be consolidated"""
        # Get all items from working memory
        items = []

        for key, value in self.working.items():
            items.append({
                "key": key,
                "content": value["content"],
                "metadata": value["metadata"],
                "timestamp": value["timestamp"]
            })

        # Sort by importance (oldest first)
        items.sort(key=lambda x: x["timestamp"])

        return items

    def _is_important(self, item: Dict[str, Any]) -> bool:
        """
        Determine if item is important enough to consolidate.

        Importance factors:
        1. Age (older = more important)
        2. Size (larger = more important)
        3. Metadata tags
        """
        # Check metadata for importance flag
        if item["metadata"].get("important", False):
            return True

        # Check for important tags
        important_tags = ["decision", "learning", "insight", "error"]
        if any(tag in item["metadata"].get("tags", []) for tag in important_tags):
            return True

        # Age-based importance
        item_time = datetime.fromisoformat(item["timestamp"])
        age_hours = (datetime.now() - item_time).total_seconds() / 3600

        if age_hours > 1:  # Older than 1 hour
            return True

        return False
```

**Integration Points:**
- Run periodically in background
- Connect to integrated memory system
- Trigger on capacity thresholds

**Files to Create:**
- `.blackbox5/engine/memory/consolidation.py` - Main implementation

**Time Estimate:** 2 days

---

## Phase 4: Production Readiness (Week 7-8)

### 4.1 CLI Tools
**Location:** `.blackbox5/engine/runtime/`

**Purpose:** Command-line tools for monitoring and management

**Implementation:**

```bash
#!/bin/bash
# view-logs.sh - View structured logs

LOG_DIR=".blackbox5/logs"
LOG_FILE="$LOG_DIR/$(date +%Y-%m-%d).log"

if [ -f "$LOG_FILE" ]; then
    if command -v jq &> /dev/null; then
        # Pretty print JSON logs
        cat "$LOG_FILE" | jq -r 'select(.level != "debug") | "\(.timestamp) [\(.level)] \(.agent_type // "system"): \(.message)"'
    else
        # Fallback: grep for important logs
        grep -E "(task\.(start|success|failed)|operation\.(start|complete|failed))" "$LOG_FILE"
    fi
else
    echo "No log file found for today"
fi
```

```bash
#!/bin/bash
# view-manifest.sh - View operation manifests

MANIFEST_DIR=".blackbox5/scratch/manifests"

if [ -z "$1" ]; then
    echo "Usage: view-manifest.sh <manifest_id>"
    echo ""
    echo "Recent manifests:"
    ls -t "$MANIFEST_DIR"/*.md 2>/dev/null | head -10 | while read f; do
        basename "$f" .md
    done
else
    MANIFEST_FILE="$MANIFEST_DIR/$1.md"
    if [ -f "$MANIFEST_FILE" ]; then
        cat "$MANIFEST_FILE"
    else
        echo "Manifest not found: $1"
    fi
fi
```

```bash
#!/bin/bash
# agent-status.sh - Check agent status

echo "Agent Status"
echo "============"
echo ""

# Check if event bus is running
if redis-cli ping > /dev/null 2>&1; then
    echo "‚úì Event Bus: Running"
else
    echo "‚úó Event Bus: Not running"
fi

# Check if ChromaDB is accessible
if curl -s http://localhost:8000/health > /dev/null 2>&1; then
    echo "‚úì Vector Store: Running"
else
    echo "‚úó Vector Store: Not running"
fi

# Check if Neo4j is accessible
if curl -s http://localhost:7474 > /dev/null 2>&1; then
    echo "‚úì Graph Database: Running"
else
    echo "‚úó Graph Database: Not running"
fi

echo ""
echo "Loaded Agents:"
python3 -c "
from pathlib import Path
import sys
sys.path.insert(0, '.blackbox5/engine')
from agents.core.AgentLoader import AgentLoader

loader = AgentLoader()
agents = loader.load_all()

for name, agent in agents.items():
    print(f\"  - {name}: {agent.__class__.__name__}\")
"
```

**Files to Create:**
- `.blackbox5/engine/runtime/view-logs.sh` - Log viewer
- `.blackbox5/engine/runtime/view-manifest.sh` - Manifest viewer
- `.blackbox5/engine/runtime/agent-status.sh` - Status checker

**Time Estimate:** 1-2 days

---

### 4.2 Testing Infrastructure
**Location:** `.blackbox5/tests/`

**Purpose:** Comprehensive testing for all components

**Implementation:**

```python
"""
Test Suite for Blackbox 5 Core Components
"""

import pytest
import asyncio
from pathlib import Path

# Test imports
import sys
sys.path.insert(0, str(Path(__file__).parent.parent / "engine"))

from core.event_bus import RedisEventBus
from core.task_router import TaskRouter, Task
from core.coordination import MultiAgentCoordinator
from core.circuit_breaker import CircuitBreaker
from memory.integrated import IntegratedMemory

class TestTaskRouter:
    """Test task routing logic"""

    def test_simple_task_routes_to_single_agent(self):
        """Simple tasks should route to single agent"""
        router = TaskRouter(mock_agents(), mock_skills())

        task = Task(
            id="test-1",
            description="Simple task",
            prompt="Do something simple",
            required_tools=[],
            domain="general",
            context={}
        )

        strategy = router.route(task)
        assert strategy.type == 'single'
        assert strategy.estimated_steps < 10

    def test_complex_task_routes_to_multi_agent(self):
        """Complex tasks should route to multi-agent"""
        router = TaskRouter(mock_agents(), mock_skills())

        task = Task(
            id="test-2",
            description="Complex task",
            prompt="This is a very long prompt with many details" * 100,
            required_tools=["tool1", "tool2", "tool3", "tool4"],
            domain="system_architecture",
            context={}
        )

        strategy = router.route(task)
        assert strategy.type == 'multi'
        assert strategy.estimated_steps >= 20

class TestCircuitBreaker:
    """Test circuit breaker functionality"""

    def test_circuit_opens_on_failures(self):
        """Circuit should open after threshold failures"""
        cb = CircuitBreaker(timeout=1, failure_threshold=3)

        def failing_function():
            raise Exception("Test failure")

        # Trigger failures
        for _ in range(3):
            try:
                cb.call(failing_function)
            except:
                pass

        # Circuit should be open
        assert cb.state == "OPEN"

        # Next call should fail immediately
        with pytest.raises(CircuitBreakerOpenError):
            cb.call(failing_function)

class TestIntegratedMemory:
    """Test integrated memory system"""

    def test_store_and_retrieve(self):
        """Should store and retrieve from all memory layers"""
        memory = IntegratedMemory()

        # Store in working memory
        memory.store(
            "Test content",
            memory_type="working",
            metadata={"key": "test"}
        )

        # Retrieve
        results = memory.retrieve("test")
        assert len(results) > 0
        assert results[0].source == "working"

class TestMultiAgentCoordinator:
    """Test multi-agent coordination"""

    @pytest.mark.asyncio
    async def test_parallel_execution(self):
        """Should execute independent tasks in parallel"""
        coordinator = MultiAgentCoordinator(
            mock_event_bus(),
            mock_agent_registry(),
            mock_circuit_breaker(),
            mock_manifest_system()
        )

        # Create task with independent subtasks
        task = create_test_task()

        # Execute
        result = await coordinator.execute(task, ExecutionStrategy(type='multi'))

        assert result is not None

# Test fixtures and helpers
def mock_agents():
    """Create mock agent registry"""
    pass

def mock_skills():
    """Create mock skill manager"""
    pass

def mock_event_bus():
    """Create mock event bus"""
    pass

# ... more fixtures
```

**Files to Create:**
- `.blackbox5/tests/test_task_router.py` - Task router tests
- `.blackbox5/tests/test_coordination.py` - Coordination tests
- `.blackbox5/tests/test_memory.py` - Memory tests
- `.blackbox5/tests/test_circuit_breaker.py` - Circuit breaker tests
- `.blackbox5/tests/conftest.py` - Test fixtures

**Time Estimate:** 3-4 days

---

## Implementation Timeline

### Week 1-2: Core Foundation
- Day 1-3: Task Router System
- Day 4-5: Manager Agent
- Day 6-7: Structured Logging
- Day 8-10: Manifest System
- Day 11-14: Testing and Integration

**Deliverables:**
- ‚úÖ Task routing based on complexity
- ‚úÖ Manager agent for coordination
- ‚úÖ Comprehensive logging
- ‚úÖ Operation tracking

### Week 3-4: Coordination System
- Day 15-18: Multi-Agent Coordinator
- Day 19-21: Parallel Execution
- Day 22-24: Memory Integration
- Day 25-28: Testing and Documentation

**Deliverables:**
- ‚úÖ Multi-agent task execution
- ‚úÖ Parallel/sequential/wave execution
- ‚úÖ Integrated memory system
- ‚úÖ End-to-end coordination

### Week 5-6: Memory Enhancement
- Day 29-31: Procedural Memory
- Day 32-33: Memory Consolidation
- Day 34-36: Semantic Memory Integration
- Day 37-42: Testing and Optimization

**Deliverables:**
- ‚úÖ Procedural memory with Redis
- ‚úÖ Automatic consolidation
- ‚úÖ Semantic memory queries
- ‚úÖ 94% task success rate

### Week 7-8: Production Readiness
- Day 43-44: CLI Tools
- Day 45-48: Testing Infrastructure
- Day 49-52: Documentation
- Day 53-56: Performance Tuning

**Deliverables:**
- ‚úÖ CLI monitoring tools
- ‚úÖ Comprehensive test suite
- ‚úÖ Complete documentation
- ‚úÖ Production-ready system

---

## Success Metrics

### Phase 1 (Week 2)
- ‚úÖ Task router correctly classifies 90%+ of tasks
- ‚úÖ Manager agent coordinates 3+ specialists
- ‚úÖ All operations logged to JSON
- ‚úÖ Manifests track all executions

### Phase 2 (Week 4)
- ‚úÖ Multi-agent tasks complete in <30s
- ‚úÖ Parallel execution shows 2x speedup
- ‚úÖ Memory integration improves retrieval
- ‚úÖ End-to-end workflows work

### Phase 3 (Week 6)
- ‚úÖ Procedural memory stores 100+ patterns
- ‚úÖ Consolidation runs automatically
- ‚úÖ Semantic queries return relevant results
- ‚úÖ 94% task success rate achieved

### Phase 4 (Week 8)
- ‚úÖ CLI tools provide visibility
- ‚úÖ Test coverage >80%
- ‚úÖ Documentation complete
- ‚úÖ System production-ready

---

## Integration Checklist

### Connect to Existing Systems

- [x] Event Bus (`.blackbox5/engine/core/event_bus.py`) ‚úÖ Already exists
- [x] Agent Loader (`.blackbox5/engine/agents/core/AgentLoader.py`) ‚úÖ Already exists
- [x] Skill Manager (`.blackbox5/engine/agents/core/SkillManager.py`) ‚úÖ Already exists
- [x] Circuit Breaker (`.blackbox5/engine/core/circuit_breaker.py`) ‚úÖ Already exists
- [x] Brain System (`.blackbox5/engine/brain/`) ‚úÖ Already exists
- [x] Memory System (`.blackbox5/engine/memory/`) ‚úÖ Already exists
- [ ] Manager Agent ‚ö†Ô∏è Needs implementation
- [ ] Task Router ‚ö†Ô∏è Needs implementation
- [ ] Multi-Agent Coordinator ‚ö†Ô∏è Needs implementation
- [ ] Integrated Memory ‚ö†Ô∏è Needs implementation
- [ ] Procedural Memory ‚ö†Ô∏è Needs implementation
- [ ] Memory Consolidation ‚ö†Ô∏è Needs implementation
- [ ] Structured Logging ‚ö†Ô∏è Needs implementation
- [ ] Manifest System ‚ö†Ô∏è Needs implementation
- [ ] CLI Tools ‚ö†Ô∏è Needs implementation
- [ ] Test Suite ‚ö†Ô∏è Needs implementation

---

## Next Steps

1. **Review this action plan** with your team
2. **Prioritize phases** based on your needs
3. **Set up development environment** (Redis, ChromaDB, Neo4j)
4. **Start with Phase 1** (Core Foundation)
5. **Test incrementally** at each phase

---

**Ready to implement!** üöÄ

This plan builds on the 70-80% you already have and adds the missing 20-30% to create a production-ready multi-agent system with 90%+ success rate in just 8 weeks.
